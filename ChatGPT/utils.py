import openai
from time import sleep
import json
import os
from config.config import MODEL_NAME, MODEL_NAME_2, OPENAI_API_KEY
openai.api_key = OPENAI_API_KEY

def safe_gpt_call(call_func, *args, **kwargs):
    for attempt in range(5):
        try:
            return call_func(*args, **kwargs)
        except openai.error.RateLimitError:
            print(f"‚è≥ Rate limit hit, sleeping 20s (attempt {attempt+1})")
            sleep(20)
    switch_model()  # Switch model if rate limit is hit
    # raise RuntimeError("Rate limit hit too many times.")
    try:
        return call_func(*args, **kwargs)
    except Exception as e:
        print(f"‚ùå GPT call failed after model switch: {e}")
        return None

def switch_model():
    global MODEL_NAME
    MODEL_NAME = MODEL_NAME_2
    print(f"üîÑ Switched model to {MODEL_NAME}")

def call_lm(messages, model=MODEL_NAME, max_tokens=512, temperature=0.7):
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        max_tokens=max_tokens,
        temperature=temperature,
    )
    prompt_tokens = response["usage"]["prompt_tokens"]
    completion_tokens = response["usage"]["completion_tokens"]
    total_tokens = prompt_tokens + completion_tokens
    print(f"üìä Tokens used: prompt={prompt_tokens}, completion={completion_tokens}")

    token_stats = []
    # Save stats to global list
    if os.path.exists("token_stats.json"):
        with open("token_stats.json", "r") as f:
            token_stats = json.load(f)
    token_stats.append({
        "model": model,
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "total_tokens": total_tokens,
        "messages": messages[-1]["content"][:100]  # just preview
    })
    with open("token_stats.json", "w") as f:
        json.dump(token_stats, f, indent=2)

    return response["choices"][0]["message"]["content"]

