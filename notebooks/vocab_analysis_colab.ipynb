{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vocabulary and POS Analysis (Fridge vs TinyStories) — Colab\n",
        "\n",
        "Цей ноутбук рахує унікальні токени, частоти, інтерсекшн/різниці та POS-статистику (дієслова, іменники, прикметники) для двох корпусів:\n",
        "- A: ваш `fridge_dataset_v1.3_clean.json`\n",
        "- B: TinyStories (через HuggingFace / локальні тексти / або як `train.bin` + `val.bin` токен-ідентифікатори)\n",
        "\n",
        "Результати зберігаються у `reports/token_stats/` (у Colab — за замовчуванням в `/content/reports/token_stats`).\n",
        "\n",
        "Кроки:\n",
        "1. Встановити залежності\n",
        "2. Налаштувати шляхи/режим завантаження TinyStories\n",
        "3. (Опційно) підключити Google Drive\n",
        "4. Запустити обчислення і подивитись результати\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Якщо працюєте в Google Colab — запустіть цю комірку\n",
        "pip -q install python-docx\n",
        "# Для завантаження TinyStories з HF\n",
        "pip -q install datasets\n",
        "# Для роботи з Excel\n",
        "!pip install xlsxwriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from typing import List, Iterable, Tuple, Dict, Optional, Generator\n",
        "\n",
        "# --- Helpers from scripts/analyze_vocab.py (inlined for Colab use) ---\n",
        "\n",
        "PCT_DECIMALS = 6  # точність відсотків у звітах\n",
        "\n",
        "\n",
        "def read_json_array_records(json_path: str) -> List[dict]:\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if not isinstance(data, list):\n",
        "        raise ValueError(f\"Expected a JSON array at {json_path}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_texts_fridge_json(json_path: str, tinyfridge=False) -> List[str]:\n",
        "    records = read_json_array_records(json_path)\n",
        "    texts: List[str] = []\n",
        "    for rec in records:\n",
        "\n",
        "        if tinyfridge:\n",
        "            instr = rec.get(\"story\") or \"\"\n",
        "        else:\n",
        "            instr = rec.get(\"instruction\") or \"\"\n",
        "        resp = rec.get(\"response\") or \"\"\n",
        "        joined = (str(instr).strip() + \"\\n\" + str(resp).strip()).strip()\n",
        "        if joined:\n",
        "            texts.append(joined)\n",
        "    return texts\n",
        "\n",
        "\n",
        "def load_texts_from_path(path: str) -> List[str]:\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"Path not found: {path}\")\n",
        "    if p.is_dir():\n",
        "        texts: List[str] = []\n",
        "        for file in p.rglob(\"*.txt\"):\n",
        "            try:\n",
        "                texts.append(file.read_text(encoding=\"utf-8\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return texts\n",
        "    if p.suffix.lower() == \".jsonl\":\n",
        "        texts = []\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                try:\n",
        "                    obj = json.loads(line)\n",
        "                    text = obj.get(\"text\") or obj.get(\"story\") or obj.get(\"content\")\n",
        "                    if text:\n",
        "                        texts.append(str(text))\n",
        "                except Exception:\n",
        "                    texts.append(line)\n",
        "        return texts\n",
        "    if p.suffix.lower() == \".json\":\n",
        "        try:\n",
        "            data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
        "            if isinstance(data, list):\n",
        "                texts: List[str] = []\n",
        "                for obj in data:\n",
        "                    if isinstance(obj, dict):\n",
        "                        text = obj.get(\"text\") or obj.get(\"story\") or obj.get(\"content\")\n",
        "                        if text:\n",
        "                            texts.append(str(text))\n",
        "                if texts:\n",
        "                    return texts\n",
        "        except Exception:\n",
        "            pass\n",
        "        return [p.read_text(encoding=\"utf-8\")]\n",
        "    return [p.read_text(encoding=\"utf-8\")]\n",
        "\n",
        "\n",
        "def load_texts_tinystories(path: str = None, use_hf: bool = False) -> List[str]:\n",
        "    if path:\n",
        "        return load_texts_from_path(path)\n",
        "    if use_hf:\n",
        "        from datasets import load_dataset\n",
        "        ds_train = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "        ds_val = load_dataset(\"roneneldan/TinyStories\", split=\"validation\")\n",
        "        texts = [r[\"text\"] for r in ds_train] + [r[\"text\"] for r in ds_val]\n",
        "        return texts\n",
        "    raise ValueError(\"TinyStories source not provided. Pass tinystories_path or use_hf=True\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def np_fromfile(path: str, dtype_str: str):\n",
        "    if dtype_str not in (\"uint16\", \"uint32\"):\n",
        "        raise ValueError(\"Unsupported dtype. Use 'uint16' or 'uint32'.\")\n",
        "    dtype = np.uint16 if dtype_str == \"uint16\" else np.uint32\n",
        "    return np.fromfile(path, dtype=dtype)\n",
        "\n",
        "\n",
        "def load_tinystories_ids_from_bins(train_bin: str, val_bin: str, dtype: str = \"uint16\"):\n",
        "    arr_train = np_fromfile(train_bin, dtype)\n",
        "    arr_val = np_fromfile(val_bin, dtype)\n",
        "    return np.concatenate([arr_train, arr_val])\n",
        "\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "def tokenize_bpe_gpt2(texts: Iterable[str]) -> List[str]:\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    tokens: List[str] = []\n",
        "    for t in texts:\n",
        "        ids = enc.encode(t, disallowed_special=())\n",
        "        tokens.extend(enc.decode_single_token_bytes(i).decode(\"utf-8\", errors=\"replace\") for i in ids)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def ids_counter_to_token_counter(id_counter: Dict[int, int]) -> Counter:\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    token_counter: Counter = Counter()\n",
        "    for tok_id, cnt in id_counter.items():\n",
        "        token_str = enc.decode_single_token_bytes(int(tok_id)).decode(\"utf-8\", errors=\"replace\")\n",
        "        token_counter[token_str] += int(cnt)\n",
        "    return token_counter\n",
        "\n",
        "\n",
        "def decode_ids_to_text_chunks(ids, chunk_tokens: int = 200_000) -> Generator[str, None, None]:\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    total_len = len(ids)\n",
        "    for i in range(0, total_len, chunk_tokens):\n",
        "        chunk = ids[i : i + chunk_tokens]\n",
        "        if not isinstance(chunk, list):\n",
        "            try:\n",
        "                chunk = chunk.tolist()\n",
        "            except Exception:\n",
        "                chunk = list(chunk)\n",
        "        yield enc.decode(chunk)\n",
        "\n",
        "\n",
        "_WORD_RE = re.compile(r\"[A-Za-z']+\")\n",
        "\n",
        "def tokenize_words(texts: Iterable[str]) -> List[str]:\n",
        "    tokens: List[str] = []\n",
        "    for t in texts:\n",
        "        tokens.extend(m.group(0).lower() for m in _WORD_RE.finditer(t))\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def compute_counter(tokens: Iterable[str]) -> Counter:\n",
        "    return Counter(tokens)\n",
        "\n",
        "\n",
        "def jaccard_percent(a_vocab: set, b_vocab: set) -> float:\n",
        "    if not a_vocab and not b_vocab:\n",
        "        return 0.0\n",
        "    inter = len(a_vocab & b_vocab)\n",
        "    union = len(a_vocab | b_vocab)\n",
        "    return 100.0 * inter / union if union else 0.0\n",
        "\n",
        "\n",
        "def ensure_dir(path: str) -> None:\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "from docx import Document\n",
        "\n",
        "def write_docx_table(path: str, title: str, subtitle_lines: List[str], headers: List[str], rows: List[List[str]]) -> None:\n",
        "    doc = Document()\n",
        "    doc.add_heading(title, level=1)\n",
        "    for line in subtitle_lines:\n",
        "        if line:\n",
        "            doc.add_paragraph(line)\n",
        "    table = doc.add_table(rows=1, cols=len(headers))\n",
        "    hdr_cells = table.rows[0].cells\n",
        "    for i, h in enumerate(headers):\n",
        "        hdr_cells[i].text = h\n",
        "    for row in rows:\n",
        "        row_cells = table.add_row().cells\n",
        "        for i, cell_val in enumerate(row):\n",
        "            row_cells[i].text = str(cell_val)\n",
        "    doc.save(path)\n",
        "\n",
        "\n",
        "import csv\n",
        "\n",
        "def write_csv(path: str, headers: List[str], rows: List[List[str]]) -> None:\n",
        "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(headers)\n",
        "        for row in rows:\n",
        "            writer.writerow(row)\n",
        "\n",
        "\n",
        "def format_pct(numerator: int, denominator: int) -> str:\n",
        "    if denominator == 0:\n",
        "        return f\"{0:.{PCT_DECIMALS}f}%\"\n",
        "    return f\"{(numerator / denominator) * 100:.{PCT_DECIMALS}f}%\"\n",
        "\n",
        "\n",
        "def top_rows_from_counter(counter: Counter, total: int) -> List[Tuple[str, int, str]]:\n",
        "    rows: List[Tuple[str, int, str]] = []\n",
        "    for token, cnt in counter.most_common():\n",
        "        rows.append((token, cnt, format_pct(cnt, total)))\n",
        "    return rows\n",
        "\n",
        "\n",
        "def compute_and_save_freq_reports(out_dir: str, label: str, counter: Counter, total: int, unit: str) -> None:\n",
        "    rows_triplets = top_rows_from_counter(counter, total)\n",
        "    headers = [\"token\", \"count\", \"percent\"]\n",
        "    rows = [[t, str(c), p] for (t, c, p) in rows_triplets]\n",
        "    ensure_dir(out_dir)\n",
        "    write_docx_table(\n",
        "        os.path.join(out_dir, f\"{unit}_freq_{label}.docx\"),\n",
        "        title=f\"Top frequencies — {label} ({unit})\",\n",
        "        subtitle_lines=[f\"Total tokens: {total}\", \"Sorted by count desc\"],\n",
        "        headers=headers,\n",
        "        rows=rows,\n",
        "    )\n",
        "    write_csv(\n",
        "        os.path.join(out_dir, f\"{unit}_freq_{label}.csv\"),\n",
        "        headers=headers,\n",
        "        rows=rows,\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_and_save_set_reports(out_dir: str, unit: str, a_label: str, b_label: str, a_counter: Counter, b_counter: Counter) -> None:\n",
        "    a_vocab = set(a_counter.keys())\n",
        "    b_vocab = set(b_counter.keys())\n",
        "    inter = a_vocab & b_vocab\n",
        "    only_a = a_vocab - b_vocab\n",
        "    only_b = b_vocab - a_vocab\n",
        "    union = a_vocab | b_vocab\n",
        "\n",
        "    jaccard = jaccard_percent(a_vocab, b_vocab)\n",
        "\n",
        "    total_a_tokens = sum(a_counter.values())\n",
        "    total_b_tokens = sum(b_counter.values())\n",
        "\n",
        "    # intersection (без percent_combined)\n",
        "    inter_rows: List[List[str]] = []\n",
        "    for tok in sorted(inter, key=lambda t: (a_counter[t] + b_counter[t]), reverse=True):\n",
        "        a_cnt = a_counter[tok]\n",
        "        b_cnt = b_counter[tok]\n",
        "        total_cnt = a_cnt + b_cnt\n",
        "        pct_a = format_pct(a_cnt, total_a_tokens)\n",
        "        pct_b = format_pct(b_cnt, total_b_tokens)\n",
        "        inter_rows.append([tok, str(total_cnt), str(a_cnt), str(b_cnt), pct_a, pct_b])\n",
        "    inter_headers = [\n",
        "        \"token\",\n",
        "        \"total_count\",\n",
        "        f\"{a_label}_count\",\n",
        "        f\"{b_label}_count\",\n",
        "        f\"{a_label}_percent\",\n",
        "        f\"{b_label}_percent\",\n",
        "    ]\n",
        "    write_docx_table(\n",
        "        os.path.join(out_dir, f\"{unit}_intersection.docx\"),\n",
        "        title=f\"Vocabulary intersection ({unit})\",\n",
        "        subtitle_lines=[\n",
        "            f\"|A|={len(a_vocab)}, |B|={len(b_vocab)}, |A∩B|={len(inter)}, |A∪B|={len(union)}\",\n",
        "            f\"Jaccard (|∩|/|∪|): {jaccard:.{PCT_DECIMALS}f}%\",\n",
        "        ],\n",
        "        headers=inter_headers,\n",
        "        rows=inter_rows,\n",
        "    )\n",
        "    write_csv(\n",
        "        os.path.join(out_dir, f\"{unit}_intersection.csv\"),\n",
        "        headers=inter_headers,\n",
        "        rows=inter_rows,\n",
        "    )\n",
        "\n",
        "    # A - B\n",
        "    diff_a_rows: List[List[str]] = []\n",
        "    for tok in sorted(only_a, key=lambda t: a_counter[t], reverse=True):\n",
        "        a_cnt = a_counter[tok]\n",
        "        pct_a = format_pct(a_cnt, total_a_tokens)\n",
        "        diff_a_rows.append([tok, pct_a, str(a_cnt)])\n",
        "    diff_a_headers = [\"token\", f\"{a_label}_percent\", f\"{a_label}_count\"]\n",
        "    write_docx_table(\n",
        "        os.path.join(out_dir, f\"{unit}_diff_{a_label}-minus-{b_label}.docx\"),\n",
        "        title=f\"Vocabulary difference A-B ({unit})\",\n",
        "        subtitle_lines=[\n",
        "            f\"|A−B|={len(only_a)} (of |A∪B|={len(union)} → {(100.0*len(only_a)/len(union) if union else 0.0):.{PCT_DECIMALS}f}%)\",\n",
        "            f\"A is {a_label}, B is {b_label}\",\n",
        "        ],\n",
        "        headers=diff_a_headers,\n",
        "        rows=diff_a_rows,\n",
        "    )\n",
        "    write_csv(\n",
        "        os.path.join(out_dir, f\"{unit}_diff_{a_label}-minus-{b_label}.csv\"),\n",
        "        headers=diff_a_headers,\n",
        "        rows=diff_a_rows,\n",
        "    )\n",
        "\n",
        "    # B - A\n",
        "    diff_b_rows: List[List[str]] = []\n",
        "    for tok in sorted(only_b, key=lambda t: b_counter[t], reverse=True):\n",
        "        b_cnt = b_counter[tok]\n",
        "        pct_b = format_pct(b_cnt, total_b_tokens)\n",
        "        diff_b_rows.append([tok, pct_b, str(b_cnt)])\n",
        "    diff_b_headers = [\"token\", f\"{b_label}_percent\", f\"{b_label}_count\"]\n",
        "    write_docx_table(\n",
        "        os.path.join(out_dir, f\"{unit}_diff_{b_label}-minus-{a_label}.docx\"),\n",
        "        title=f\"Vocabulary difference B-A ({unit})\",\n",
        "        subtitle_lines=[\n",
        "            f\"|B−A|={len(only_b)} (of |A∪B|={len(union)} → {(100.0*len(only_b)/len(union) if union else 0.0):.{PCT_DECIMALS}f}%)\",\n",
        "            f\"B is {b_label}, A is {a_label}\",\n",
        "        ],\n",
        "        headers=diff_b_headers,\n",
        "        rows=diff_b_rows,\n",
        "    )\n",
        "    write_csv(\n",
        "        os.path.join(out_dir, f\"{unit}_diff_{b_label}-minus-{a_label}.csv\"),\n",
        "        headers=diff_b_headers,\n",
        "        rows=diff_b_rows,\n",
        "    )\n",
        "\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def pos_counts(texts: List[str]) -> Dict[str, int]:\n",
        "    # Try spaCy; fallback NLTK\n",
        "    counts = {\"VERB\": 0, \"NOUN\": 0, \"ADJ\": 0}\n",
        "    print(f\"Обробляється {len(texts)} текстів...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        import spacy\n",
        "        spacy.require_gpu()\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "            nlp.to(\"cuda\")\n",
        "        except Exception:\n",
        "            from spacy.cli import download as spacy_download\n",
        "            spacy_download(\"en_core_web_sm\")\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "            nlp.to(\"cuda\")\n",
        "        for doc in tqdm(nlp.pipe(texts, disable=[\"ner\", \"parser\"]), \n",
        "                       total=len(texts), desc=\"spaCy POS tagging\"):\n",
        "            for token in doc:\n",
        "                if token.pos_ in counts:\n",
        "                    counts[token.pos_] += 1\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"spaCy завершено за {elapsed:.1f}s\")\n",
        "        return counts\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import nltk\n",
        "        try:\n",
        "            nltk.data.find(\"tokenizers/punkt\")\n",
        "        except LookupError:\n",
        "            nltk.download(\"punkt\", quiet=True)\n",
        "        try:\n",
        "            nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
        "        except LookupError:\n",
        "            nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
        "        try:\n",
        "            nltk.data.find(\"taggers/averaged_perceptron_tagger_eng\")\n",
        "        except LookupError:\n",
        "            nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
        "        from nltk import word_tokenize, pos_tag\n",
        "        for text in tqdm(texts, desc=\"NLTK POS tagging\"):\n",
        "            tokens = nltk.word_tokenize(text)\n",
        "            tags = nltk.pos_tag(tokens)\n",
        "            for _, tag in tags:\n",
        "                if tag.startswith(\"VB\"):\n",
        "                    counts[\"VERB\"] += 1\n",
        "                elif tag.startswith(\"NN\"):\n",
        "                    counts[\"NOUN\"] += 1\n",
        "                elif tag.startswith(\"JJ\"):\n",
        "                    counts[\"ADJ\"] += 1\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"NLTK завершено за {elapsed:.1f}s\")\n",
        "        return counts\n",
        "    except Exception:\n",
        "        return counts\n",
        "\n",
        "\n",
        "def write_pos_docx(out_path: str, a_label: str, b_label: str, a_counts: Dict[str, int], b_counts: Dict[str, int]) -> None:\n",
        "    total_a = sum(a_counts.values())\n",
        "    total_b = sum(b_counts.values())\n",
        "    headers = [\"class\", f\"{a_label}_count\", f\"{a_label}_percent\", f\"{b_label}_count\", f\"{b_label}_percent\"]\n",
        "    rows: List[List[str]] = []\n",
        "    def pct(v, tot):\n",
        "        return f\"{(v/tot*100):.{PCT_DECIMALS}f}%\" if tot else f\"{0:.{PCT_DECIMALS}f}%\"\n",
        "    for cls in [\"VERB\", \"NOUN\", \"ADJ\"]:\n",
        "        rows.append([\n",
        "            cls,\n",
        "            str(a_counts.get(cls, 0)),\n",
        "            pct(a_counts.get(cls, 0), total_a),\n",
        "            str(b_counts.get(cls, 0)),\n",
        "            pct(b_counts.get(cls, 0), total_b),\n",
        "        ])\n",
        "    write_docx_table(\n",
        "        out_path,\n",
        "        title=\"POS counts (VERB/NOUN/ADJ)\",\n",
        "        subtitle_lines=[f\"Totals — {a_label}: {total_a}, {b_label}: {total_b}\"],\n",
        "        headers=headers,\n",
        "        rows=rows,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Налаштування\n",
        "USE_HF = False               # True: завантажити TinyStories з HuggingFace\n",
        "TINYSTORIES_PATH = None      # шлях до тек/файлів TinyStories текстом (якщо не HF і не .bin)\n",
        "TINYSTORIES_BIN_TRAIN = None # шлях до train.bin (якщо .bin-режим)\n",
        "TINYSTORIES_BIN_VAL = None   # шлях до val.bin (якщо .bin-режим)\n",
        "TINYSTORIES_BIN_DTYPE = 'uint16'  # 'uint16' або 'uint32'\n",
        "DECODE_BIN_FOR_WORDS_POS = False  # декодувати .bin назад у текст для word/POS (повільніше)\n",
        "\n",
        "FRIDGE_JSON = '../stories.json'\n",
        "OUT_DIR = 'reports/token_stats'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обробляється 85994 текстів...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "NLTK POS tagging: 100%|██████████| 85994/85994 [16:13<00:00, 88.35it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK завершено за 973.3s\n"
          ]
        }
      ],
      "source": [
        "# Завантаження корпусів і підрахунки\n",
        "\n",
        "a_label = 'tinyfridge'\n",
        "# b_label = 'tinystories'\n",
        "\n",
        "# A: Fridge JSON\n",
        "a_texts = load_texts_fridge_json(FRIDGE_JSON)\n",
        "\n",
        "# B: TinyStories — HF / текст / або .bin\n",
        "# b_texts: Optional[List[str]] = None\n",
        "# b_ids = None\n",
        "# if TINYSTORIES_BIN_TRAIN and TINYSTORIES_BIN_VAL:\n",
        "#     b_ids = load_tinystories_ids_from_bins(TINYSTORIES_BIN_TRAIN, TINYSTORIES_BIN_VAL, dtype=TINYSTORIES_BIN_DTYPE)\n",
        "# elif TINYSTORIES_PATH or USE_HF:\n",
        "#     b_texts = load_texts_tinystories(TINYSTORIES_PATH, use_hf=USE_HF)\n",
        "# else:\n",
        "#     raise ValueError('Вкажіть джерело TinyStories: USE_HF=True, або TINYSTORIES_PATH, або шляхи до .bin')\n",
        "\n",
        "# # BPE токени\n",
        "a_bpe = tokenize_bpe_gpt2(a_texts)\n",
        "a_bpe_counter = compute_counter(a_bpe)\n",
        "\n",
        "# if b_ids is not None:\n",
        "#     # Рахуємо частоти по id і мапимо до BPE-токенів\n",
        "#     unique_ids, counts = np.unique(b_ids, return_counts=True)\n",
        "#     id_counter = {int(i): int(c) for i, c in zip(unique_ids.tolist(), counts.tolist())}\n",
        "#     b_bpe_counter = ids_counter_to_token_counter(id_counter)\n",
        "#     total_b_bpe = sum(b_bpe_counter.values())\n",
        "#     # Для TinyStories слів/POS треба або декодувати, або пропустити\n",
        "#     if DECODE_BIN_FOR_WORDS_POS:\n",
        "#         decoded_chunks = list(decode_ids_to_text_chunks(b_ids))\n",
        "#         b_texts_for_pos = decoded_chunks\n",
        "#     else:\n",
        "#         b_texts_for_pos = []\n",
        "# else:\n",
        "#     assert b_texts is not None\n",
        "#     b_bpe = tokenize_bpe_gpt2(b_texts)\n",
        "#     b_bpe_counter = compute_counter(b_bpe)\n",
        "#     total_b_bpe = len(b_bpe)\n",
        "#     b_texts_for_pos = b_texts\n",
        "\n",
        "# # Звіти по множинах (BPE)\n",
        "ensure_dir(OUT_DIR)\n",
        "# compute_and_save_set_reports(OUT_DIR, 'bpe', a_label, b_label, a_bpe_counter, b_bpe_counter)\n",
        "\n",
        "# # Частоти (BPE)\n",
        "# compute_and_save_freq_reports(OUT_DIR, b_label, b_bpe_counter, total_b_bpe, unit='bpe')\n",
        "compute_and_save_freq_reports(OUT_DIR, a_label, a_bpe_counter, len(a_bpe), unit='bpe')\n",
        "\n",
        "# # Word/POS\n",
        "a_words = tokenize_words(a_texts)\n",
        "a_word_counter = compute_counter(a_words)\n",
        "\n",
        "# if len(b_texts_for_pos) > 0:\n",
        "#     b_words = tokenize_words(b_texts_for_pos)\n",
        "#     b_word_counter = compute_counter(b_words)\n",
        "# else:\n",
        "#     b_words = []\n",
        "#     b_word_counter = Counter()\n",
        "\n",
        "# # Частоти (word)\n",
        "# if len(b_words) > 0:\n",
        "#     compute_and_save_freq_reports(OUT_DIR, b_label, b_word_counter, len(b_words), unit='word')\n",
        "compute_and_save_freq_reports(OUT_DIR, a_label, a_word_counter, len(a_words), unit='word')\n",
        "\n",
        "# POS\n",
        "a_pos = pos_counts(a_texts)\n",
        "# b_pos = pos_counts(b_texts_for_pos) if len(b_texts_for_pos) > 0 else {\"VERB\": 0, \"NOUN\": 0, \"ADJ\": 0}\n",
        "# write_pos_docx(os.path.join(OUT_DIR, 'pos_counts.docx'), a_label, b_label, a_counts=a_pos, b_counts=b_pos)\n",
        "\n",
        "# # Додаткові резюме\n",
        "with open(os.path.join(OUT_DIR, 'bpe_unique_counts.txt'), 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"{a_label} unique tokens (BPE): {len(a_bpe_counter)}\\n\")\n",
        "    # f.write(f\"{b_label} unique tokens (BPE): {len(b_bpe_counter)}\\n\")\n",
        "\n",
        "with open(os.path.join(OUT_DIR, 'bpe_sets_summary.txt'), 'w', encoding='utf-8') as f:\n",
        "    a_vocab = set(a_bpe_counter.keys())\n",
        "    # b_vocab = set(b_bpe_counter.keys())\n",
        "    # inter = len(a_vocab & b_vocab)\n",
        "    # union = len(a_vocab | b_vocab)\n",
        "    # jacc = 100.0 * inter / union if union else 0.0\n",
        "    # f.write(f\"|A|={len(a_vocab)}, |B|={len(b_vocab)}, |A∩B|={inter}, |A∪B|={union}\\n\")\n",
        "    f.write(f\"|A|={len(a_vocab)}\")\n",
        "    # f.write(f\"Jaccard (|∩|/|∪|): {jacc:.{PCT_DECIMALS}f}%\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Додатковий аналіз: унікальні слова та частоти усередині POS-класів\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def pos_tag_texts_spacy(texts: List[str]):\n",
        "    try:\n",
        "        import spacy\n",
        "        try:\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "        except Exception:\n",
        "            from spacy.cli import download as spacy_download\n",
        "            spacy_download(\"en_core_web_sm\")\n",
        "            nlp = spacy.load(\"en_core_web_sm\")\n",
        "        for doc in nlp.pipe(texts, disable=[\"ner\", \"parser\"]):\n",
        "            yield [(t.text, t.lemma_, t.pos_) for t in doc]\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def build_pos_vocab_and_freq(tagged_docs, allowed_pos=(\"NOUN\", \"VERB\", \"ADJ\")):\n",
        "    # Повертає по кожному POS: Counter лем, загальна кількість токенів цього POS\n",
        "    pos_to_counter = {p: Counter() for p in allowed_pos}\n",
        "    pos_to_total = {p: 0 for p in allowed_pos}\n",
        "    for doc in tagged_docs:\n",
        "        for text, lemma, pos in doc:\n",
        "            if pos in pos_to_counter:\n",
        "                lemma_norm = lemma.lower()\n",
        "                pos_to_counter[pos][lemma_norm] += 1\n",
        "                pos_to_total[pos] += 1\n",
        "    return pos_to_counter, pos_to_total\n",
        "\n",
        "\n",
        "def write_pos_vocab_docx(out_path: str, label: str, pos_to_counter: Dict[str, Counter], pos_to_total: Dict[str, int], top_k: int = 200):\n",
        "    # Формат: POS, unique_count, top lemma, percent_in_pos, count\n",
        "    rows: List[List[str]] = []\n",
        "    headers = [\"pos\", \"unique_in_pos\", \"lemma\", \"percent_in_pos\", \"count\"]\n",
        "    for pos_cls in (\"NOUN\", \"VERB\", \"ADJ\"):\n",
        "        counter = pos_to_counter.get(pos_cls, Counter())\n",
        "        total = pos_to_total.get(pos_cls, 0)\n",
        "        unique = len(counter)\n",
        "        for lemma, cnt in counter.most_common(top_k):\n",
        "            rows.append([pos_cls, str(unique), lemma, format_pct(cnt, total), str(cnt)])\n",
        "    write_docx_table(\n",
        "        out_path,\n",
        "        title=f\"POS lemma frequencies — {label}\",\n",
        "        subtitle_lines=[\"percent_in_pos = частка відносно всіх токенів цього POS у корпусі\"],\n",
        "        headers=headers,\n",
        "        rows=rows,\n",
        "    )\n",
        "\n",
        "\n",
        "# Обчислюємо POS-леми і частоти для обох корпусів (якщо є тексти)\n",
        "fridge_tagged = list(pos_tag_texts_spacy(a_texts))\n",
        "if len(b_texts_for_pos) > 0:\n",
        "    tinystories_tagged = list(pos_tag_texts_spacy(b_texts_for_pos))\n",
        "else:\n",
        "    tinystories_tagged = []\n",
        "\n",
        "# Fridge POS-леми\n",
        "if fridge_tagged:\n",
        "    fridge_pos_counters, fridge_pos_totals = build_pos_vocab_and_freq(fridge_tagged)\n",
        "    write_pos_vocab_docx(os.path.join(OUT_DIR, 'pos_lemmas_fridge.docx'), 'fridge', fridge_pos_counters, fridge_pos_totals, top_k=500)\n",
        "\n",
        "# TinyStories POS-леми (якщо були тексти)\n",
        "if tinystories_tagged:\n",
        "    ts_pos_counters, ts_pos_totals = build_pos_vocab_and_freq(tinystories_tagged)\n",
        "    write_pos_vocab_docx(os.path.join(OUT_DIR, 'pos_lemmas_tinystories.docx'), 'tinystories', ts_pos_counters, ts_pos_totals, top_k=500)\n",
        "\n",
        "print('Готово: додаткові POS-леми збережено (за наявності текстів).')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
