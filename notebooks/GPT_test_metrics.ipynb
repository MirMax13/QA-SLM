{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a9f843",
   "metadata": {
    "executionInfo": {
     "elapsed": 9458,
     "status": "ok",
     "timestamp": 1753204059134,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "f0a9f843"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oTIJaMb46vx8",
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1753204078270,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "oTIJaMb46vx8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "FPK7Np-0Sc-h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1753204081690,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "FPK7Np-0Sc-h",
    "outputId": "cc50a54e-5c65-4a46-bb7c-bde5bf5736f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "U2fCyrEU6XCW",
   "metadata": {
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1753204083841,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "U2fCyrEU6XCW"
   },
   "outputs": [],
   "source": [
    "numb = 50257\n",
    "# 50257\n",
    "config = GPTConfig(\n",
    "    vocab_size=numb,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a9e3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступна: True\n",
      "CUDA версія: 12.1\n",
      "CUDA працює нормально\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Встановіть debugging режим\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Перевірте CUDA\n",
    "print(f\"CUDA доступна: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA версія: {torch.version.cuda}\")\n",
    "\n",
    "try:\n",
    "    # Створіть тестовий тензор\n",
    "    test_tensor = torch.tensor([1.0]).cuda()\n",
    "    print(\"CUDA працює нормально\")\n",
    "    del test_tensor\n",
    "except Exception as e:\n",
    "    print(f\"CUDA проблема: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cab9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_model_load(model, path, numb):\n",
    "    try:\n",
    "        # Завантажте на CPU спочатку\n",
    "        print(\"Завантажую на CPU...\")\n",
    "        checkpoint = torch.load(f\"{path}/gpt_1.3_new_gpt_50ep.pt\", map_location='cpu')\n",
    "        # checkpoint = torch.load(f\"{path}/TinyStories_{numb}.pt\", map_location='cpu')\n",
    "        # checkpoint = torch.load(f\"{path}/114mb_20000.pt\", map_location='cpu')\n",
    "        \n",
    "        # Завантажте state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State dict завантажено\")\n",
    "        \n",
    "        # Очистіть checkpoint з пам'яті\n",
    "        del checkpoint\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Поступово перенесіть на GPU\n",
    "        print(\"Переношу на GPU...\")\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Модель успішно завантажена на GPU\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Помилка завантаження: {e}\")\n",
    "        print(\"Залишаю модель на CPU\")\n",
    "        model.eval()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nz4Txftb6esb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1753204155915,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "nz4Txftb6esb",
    "outputId": "3a4baa7c-99d6-4d2f-ba98-4abcf2b757ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завантажую на CPU...\n",
      "State dict завантажено\n",
      "Переношу на GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matsk\\AppData\\Local\\Temp\\ipykernel_17036\\2764060940.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{path}/gpt_1.3_new_gpt_50ep.pt\", map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успішно завантажена на GPU\n",
      "Модель на пристрої: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = safe_model_load(model, \"../models\", numb)\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Модель на пристрої: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8357ed5d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1596,
     "status": "ok",
     "timestamp": 1753204162535,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "8357ed5d"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "block_size = config.block_size  # =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf717e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "import random as _random\n",
    "_random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e484896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(instruction, max_new_tokens=100, temperature=0.6, top_k=40):\n",
    "    prompt = f\"question: {instruction}\\nanswer:\"\n",
    "    input_ids = enc.encode_ordinary(prompt)\n",
    "    input_ids = input_ids[:config.block_size]\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long)[None].to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            if input_tensor.shape[1] > config.block_size:\n",
    "                input_tensor = input_tensor[:, -config.block_size:]\n",
    "\n",
    "            logits, _ = model(input_tensor)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                values, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < values[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_tensor = torch.cat([input_tensor, next_token], dim=1)\n",
    "\n",
    "            # Зупиняємо генерацію, якщо згенеровано токен <|END|> (50256)\n",
    "            if next_token.item() == 50256:\n",
    "                break\n",
    "\n",
    "    output_tokens = input_tensor[0].tolist()\n",
    "    generated = enc.decode(output_tokens[len(input_ids):])\n",
    "    return generated.strip().replace(\"<|END|><|endoftext|>\", \"\")  # Видаляємо <|END|> із виводу\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f60968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You turned on PowerCool in the refrigerator. What happens next? A) The refrigerator will always stay at maximum cooling. B) After a few hours, it returns to the previous temperature. C) The refrigerator stops working completely.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/hella_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"instruction\": item[\"instruction\"], \"response\": item[\"response\"]} for item in data]\n",
    "print(input_pairs[0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e6047a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: You turned on PowerCool in the refrigerator. What happens next? A) The refrigerator will always stay at maximum cooling. B) After a few hours, it returns to the previous temperature. C) The refrigerator stops working completely.\n",
      "Generated: For specific instructions on specialized compartments, it is recommended to keep the freezer’s ideal for quickly freezing bulk items, ensuring airtight containers to keep the temperature stable.\n",
      "Expected: B\n",
      "--------------------------------------------------\n",
      "Instruction: A person left the fridge door open for a long time. What will likely happen? A) The temperature inside will rise and food may spoil. B) The refrigerator will automatically close the door. C) The temperature will drop further.\n",
      "Generated: To ensure optimal food safety and potential contamination, ensure the door isn’t left open for extended periods to keep the temperature stable; routinely clean surfaces that come into contact with food; wash water tanks if unused for 48 hours and flush the water system if not used for five days; store raw meat and fish separately to avoid cross-contamination; use the two-star<|endoftext|>\n",
      "Expected: A\n",
      "--------------------------------------------------\n",
      "Instruction: The fridge was placed directly against the wall. What is the result? A) Ventilation may be blocked, increasing energy use. B) The fridge will run quieter. C) It will last longer.\n",
      "Generated: To ensure correct operation and prevent freezer to freeze these energy on one, store each type of food, and ensure the appliance resets are often required for up to 2 weeks.\n",
      "Expected: A\n",
      "--------------------------------------------------\n",
      "Instruction: The power cord of the fridge is damaged. What should be done? A) Replace the cord only at a service center. B) Wrap it with tape and keep using it. C) The fridge can work without a cord.\n",
      "Generated: Only specific to avoid electrical hazards or operational problems, and direct sunlight. Following these guidelines helps ensure safety and prevents harm.\n",
      "Expected: A\n",
      "--------------------------------------------------\n",
      "Instruction: Raw meat was placed on the top shelf without a container. What may happen? A) Other food may get contaminated. B) The meat will last longer. C) The meat will freeze instantly.\n",
      "Generated: To ensure moisture, maintain both safety and proper functioning.\n",
      "Expected: A\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pair in input_pairs:\n",
    "    instruction = pair[\"instruction\"]\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Expected:\", pair[\"response\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "416c24e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How should eggs be stored in the fridge – in the door or in a carton on a shelf?\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/piqa_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"instruction\": item[\"instruction\"], \"response\": item[\"response\"]} for item in data]\n",
    "print(input_pairs[0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "020110cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: How should eggs be stored in the fridge – in the door or in a carton on a shelf?\n",
      "Generated: Leaving the door open for extended times can raise the temperature considerably, potentially compromising food safety and reducing the appliance's efficiency.\n",
      "Expected: in a carton on a shelf\n",
      "--------------------------------------------------\n",
      "Instruction: What is the safe way to clean the fridge – unplug it first or wash inside with a water jet?\n",
      "Generated: I apologize, but I am a refrigerator assistant and cannot help with water rules.\n",
      "Expected: unplug it first\n",
      "--------------------------------------------------\n",
      "Instruction: How can frost be avoided – by opening the door often or by ensuring proper ventilation and temperature?\n",
      "Generated: When installing, make sure not to place the refrigerator door upside down, since this can result in a multi-socket adapter or operational problems, making it important to keep the appliance stable and undamaged.\n",
      "Expected: by ensuring proper ventilation and temperature\n",
      "--------------------------------------------------\n",
      "Instruction: What should be done when leaving the fridge unused for a long time – keep it running with doors closed or turn it off, clean it, and leave doors open?\n",
      "Generated: To avoid electric shock when cleaning, never spray water straight onto the refrigerator. Always disconnect the power cord beforehand and make sure that all contact points and power terminals are completely dry. Use cleaning agents that are safe and do not create electrical dangers.\n",
      "Expected: turn it off, clean it, and leave doors open\n",
      "--------------------------------------------------\n",
      "Instruction: What is the safe way to deal with a damaged power cord – replace it at a service center or wrap it with tape and keep using it?\n",
      "Generated: To ensure safe and effective use of your appliance, you should read the manual thoroughly beforehand. Doing so will give you the knowledge needed for proper setup and care.\n",
      "Expected: replace it at a service center\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pair in input_pairs:\n",
    "    instruction = pair[\"instruction\"]\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Expected:\", pair[\"response\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0336f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can the refrigerator be transported lying down?\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/boolq_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"instruction\": item[\"instruction\"], \"response\": item[\"response\"]} for item in data]\n",
    "print(input_pairs[0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bb25c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Can the refrigerator be transported lying down?\n",
      "Generated: I apologize, but I am a refrigerator assistant and cannot help with athletic guidance.\n",
      "Expected: no\n",
      "--------------------------------------------------\n",
      "Instruction: Should the refrigerator only be connected to a grounded outlet?\n",
      "Generated: The refrigerator should only be used for storing food, and storing volatile or flammable chemicals inside is prohibited. Doing so could trigger fires or explosions, making it important to follow this safety rule.\n",
      "Expected: yes\n",
      "--------------------------------------------------\n",
      "Instruction: Can raw meat be stored on a shelf without a container?\n",
      "Generated: Store raw meat and fish in suitable containers helps prevent them from touching other foods or leaking fluids, which is essential for preventing cross-contamination and ensuring food safety.\n",
      "Expected: no\n",
      "--------------------------------------------------\n",
      "Instruction: Does PowerCool turn off automatically after a few hours?\n",
      "Generated: I apologize, but I am a refrigerator assistant and cannot help with medical advice.\n",
      "Expected: yes\n",
      "--------------------------------------------------\n",
      "Instruction: Can old hose-sets be reused for the water connection?\n",
      "Generated: The appliance ought to be limited to potable water supplies to maintain water safety and quality within the device.\n",
      "Expected: no\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pair in input_pairs:\n",
    "    instruction = pair[\"instruction\"]\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Expected:\", pair[\"response\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b425741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user placed raw meat in a container and put it in the fridge so _ would not contaminate other food. meat container\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/WinoGrande_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"instruction\": item[\"instruction\"], \"response\": item[\"response\"]} for item in data]\n",
    "print(input_pairs[0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac97a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: The user placed raw meat in a container and put it in the fridge so _ would not contaminate other food. meat container\n",
      "Generated: Store raw meat and fish in suitable containers to prevent their juices from dripping onto other foods and to maintain safe, contamination-free storage.\n",
      "Expected: meat\n",
      "--------------------------------------------------\n",
      "Instruction: The child opened the fridge door, although _ was heavy. child door\n",
      "Generated: Do not hang on the refrigerator door, as this may lead to benzene or operational problems, ensuring the door doesn't pinch the cable.\n",
      "Expected: door\n",
      "--------------------------------------------------\n",
      "Instruction: The technician checked the compressor because _ was noisy. technician compressor\n",
      "Generated: I apologize, but I am a refrigerator assistant and cannot help with historical facts.\n",
      "Expected: compressor\n",
      "--------------------------------------------------\n",
      "Instruction: The user put a bottle in the fridge door because _ was tall. bottle door\n",
      "Generated: To avoid creating a variety of package sizes. This adjustability gives users greater flexibility to organize and utilize the refrigerator’s storage space for assorted food container sizes and types.\n",
      "Expected: bottle\n",
      "--------------------------------------------------\n",
      "Instruction: The mother gave the manual to the child so _ would know how to use the fridge. mother child\n",
      "Generated: To maximize safety instructions can be kept refrigerated, it must be turned off, defrosted, cleaned, and left open to dry. This prevents mold growth and maintains the appliance's cleanliness and functionality.\n",
      "Expected: child\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pair in input_pairs:\n",
    "    instruction = pair[\"instruction\"]\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Expected:\", pair[\"response\"])\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
