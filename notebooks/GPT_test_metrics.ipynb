{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a9f843",
   "metadata": {
    "executionInfo": {
     "elapsed": 9458,
     "status": "ok",
     "timestamp": 1753204059134,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "f0a9f843"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oTIJaMb46vx8",
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1753204078270,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "oTIJaMb46vx8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "FPK7Np-0Sc-h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1753204081690,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "FPK7Np-0Sc-h",
    "outputId": "cc50a54e-5c65-4a46-bb7c-bde5bf5736f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "U2fCyrEU6XCW",
   "metadata": {
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1753204083841,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "U2fCyrEU6XCW"
   },
   "outputs": [],
   "source": [
    "numb = 50257\n",
    "# 50257\n",
    "config = GPTConfig(\n",
    "    vocab_size=numb,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a9e3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступна: True\n",
      "CUDA версія: 12.1\n",
      "CUDA працює нормально\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Встановіть debugging режим\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Перевірте CUDA\n",
    "print(f\"CUDA доступна: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA версія: {torch.version.cuda}\")\n",
    "\n",
    "try:\n",
    "    # Створіть тестовий тензор\n",
    "    test_tensor = torch.tensor([1.0]).cuda()\n",
    "    print(\"CUDA працює нормально\")\n",
    "    del test_tensor\n",
    "except Exception as e:\n",
    "    print(f\"CUDA проблема: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cab9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_model_load(model, path, numb):\n",
    "    try:\n",
    "        # Завантажте на CPU спочатку\n",
    "        print(\"Завантажую на CPU...\")\n",
    "        checkpoint = torch.load(f\"{path}/gpt_1.3_new_gpt_50ep.pt\", map_location='cpu')\n",
    "        # checkpoint = torch.load(f\"{path}/TinyStories_{numb}.pt\", map_location='cpu')\n",
    "        # checkpoint = torch.load(f\"{path}/114mb_20000.pt\", map_location='cpu')\n",
    "        \n",
    "        # Завантажте state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State dict завантажено\")\n",
    "        \n",
    "        # Очистіть checkpoint з пам'яті\n",
    "        del checkpoint\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Поступово перенесіть на GPU\n",
    "        print(\"Переношу на GPU...\")\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Модель успішно завантажена на GPU\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Помилка завантаження: {e}\")\n",
    "        print(\"Залишаю модель на CPU\")\n",
    "        model.eval()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nz4Txftb6esb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1753204155915,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "nz4Txftb6esb",
    "outputId": "3a4baa7c-99d6-4d2f-ba98-4abcf2b757ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завантажую на CPU...\n",
      "State dict завантажено\n",
      "Переношу на GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matsk\\AppData\\Local\\Temp\\ipykernel_17036\\2764060940.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{path}/gpt_1.3_new_gpt_50ep.pt\", map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успішно завантажена на GPU\n",
      "Модель на пристрої: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = safe_model_load(model, \"../models\", numb)\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Модель на пристрої: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8357ed5d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1596,
     "status": "ok",
     "timestamp": 1753204162535,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "8357ed5d"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "block_size = config.block_size  # =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf717e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "import random as _random\n",
    "_random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e484896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(instruction, max_new_tokens=100, temperature=0.6, top_k=40):\n",
    "    prompt = f\"question: {instruction}\\nanswer:\"\n",
    "    input_ids = enc.encode_ordinary(prompt)\n",
    "    input_ids = input_ids[:config.block_size]\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long)[None].to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            if input_tensor.shape[1] > config.block_size:\n",
    "                input_tensor = input_tensor[:, -config.block_size:]\n",
    "\n",
    "            logits, _ = model(input_tensor)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                values, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < values[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_tensor = torch.cat([input_tensor, next_token], dim=1)\n",
    "\n",
    "            # Зупиняємо генерацію, якщо згенеровано токен <|END|> (50256)\n",
    "            if next_token.item() == 50256:\n",
    "                break\n",
    "\n",
    "    output_tokens = input_tensor[0].tolist()\n",
    "    generated = enc.decode(output_tokens[len(input_ids):])\n",
    "    return generated.strip().replace(\"<|END|><|endoftext|>\", \"\")  # Видаляємо <|END|> із виводу\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f60968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You turned on PowerCool in the refrigerator. What happens next? A) The refrigerator will always stay at maximum cooling. B) After a few hours, it returns to the previous temperature. C) The refrigerator stops working completely.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/hella_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"instruction\": item[\"instruction\"], \"response\": item[\"response\"]} for item in data]\n",
    "print(input_pairs[0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e6047a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: You turned on PowerCool in the refrigerator. What happens next? A) The refrigerator will always stay at maximum cooling. B) After a few hours, it returns to the previous temperature. C) The refrigerator stops working completely.\n",
      "Generated: For specific instructions on specialized compartments, it is recommended to keep the freezer’s ideal for quickly freezing bulk items, ensuring airtight containers to keep the temperature stable.\n",
      "Expected: B\n",
      "--------------------------------------------------\n",
      "Instruction: A person left the fridge door open for a long time. What will likely happen? A) The temperature inside will rise and food may spoil. B) The refrigerator will automatically close the door. C) The temperature will drop further.\n",
      "Generated: To ensure optimal food safety and potential contamination, ensure the door isn’t left open for extended periods to keep the temperature stable; routinely clean surfaces that come into contact with food; wash water tanks if unused for 48 hours and flush the water system if not used for five days; store raw meat and fish separately to avoid cross-contamination; use the two-star<|endoftext|>\n",
      "Expected: A\n",
      "--------------------------------------------------\n",
      "Instruction: The fridge was placed directly against the wall. What is the result? A) Ventilation may be blocked, increasing energy use. B) The fridge will run quieter. C) It will last longer.\n",
      "Generated: To ensure correct operation and prevent freezer to freeze these energy on one, store each type of food, and ensure the appliance resets are often required for up to 2 weeks.\n",
      "Expected: A\n",
      "--------------------------------------------------\n",
      "Instruction: The power cord of the fridge is damaged. What should be done? A) Replace the cord only at a service center. B) Wrap it with tape and keep using it. C) The fridge can work without a cord.\n",
      "Generated: Only specific to avoid electrical hazards or operational problems, and direct sunlight. Following these guidelines helps ensure safety and prevents harm.\n",
      "Expected: A\n",
      "--------------------------------------------------\n",
      "Instruction: Raw meat was placed on the top shelf without a container. What may happen? A) Other food may get contaminated. B) The meat will last longer. C) The meat will freeze instantly.\n",
      "Generated: To ensure moisture, maintain both safety and proper functioning.\n",
      "Expected: A\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pair in input_pairs:\n",
    "    instruction = pair[\"instruction\"]\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Expected:\", pair[\"response\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "416c24e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How should eggs be stored in the fridge – in the door or in a carton on a shelf?\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/piqa_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"instruction\": item[\"instruction\"], \"response\": item[\"response\"]} for item in data]\n",
    "print(input_pairs[0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "020110cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: How should eggs be stored in the fridge – in the door or in a carton on a shelf?\n",
      "Generated: Leaving the door open for extended times can raise the temperature considerably, potentially compromising food safety and reducing the appliance's efficiency.\n",
      "Expected: in a carton on a shelf\n",
      "--------------------------------------------------\n",
      "Instruction: What is the safe way to clean the fridge – unplug it first or wash inside with a water jet?\n",
      "Generated: I apologize, but I am a refrigerator assistant and cannot help with water rules.\n",
      "Expected: unplug it first\n",
      "--------------------------------------------------\n",
      "Instruction: How can frost be avoided – by opening the door often or by ensuring proper ventilation and temperature?\n",
      "Generated: When installing, make sure not to place the refrigerator door upside down, since this can result in a multi-socket adapter or operational problems, making it important to keep the appliance stable and undamaged.\n",
      "Expected: by ensuring proper ventilation and temperature\n",
      "--------------------------------------------------\n",
      "Instruction: What should be done when leaving the fridge unused for a long time – keep it running with doors closed or turn it off, clean it, and leave doors open?\n",
      "Generated: To avoid electric shock when cleaning, never spray water straight onto the refrigerator. Always disconnect the power cord beforehand and make sure that all contact points and power terminals are completely dry. Use cleaning agents that are safe and do not create electrical dangers.\n",
      "Expected: turn it off, clean it, and leave doors open\n",
      "--------------------------------------------------\n",
      "Instruction: What is the safe way to deal with a damaged power cord – replace it at a service center or wrap it with tape and keep using it?\n",
      "Generated: To ensure safe and effective use of your appliance, you should read the manual thoroughly beforehand. Doing so will give you the knowledge needed for proper setup and care.\n",
      "Expected: replace it at a service center\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pair in input_pairs:\n",
    "    instruction = pair[\"instruction\"]\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Expected:\", pair[\"response\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0336f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can the refrigerator be transported lying down?\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/boolq_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"instruction\": item[\"instruction\"], \"response\": item[\"response\"]} for item in data]\n",
    "print(input_pairs[0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bb25c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Can the refrigerator be transported lying down?\n",
      "Generated: I apologize, but I am a refrigerator assistant and cannot help with athletic guidance.\n",
      "Expected: no\n",
      "--------------------------------------------------\n",
      "Instruction: Should the refrigerator only be connected to a grounded outlet?\n",
      "Generated: The refrigerator should only be used for storing food, and storing volatile or flammable chemicals inside is prohibited. Doing so could trigger fires or explosions, making it important to follow this safety rule.\n",
      "Expected: yes\n",
      "--------------------------------------------------\n",
      "Instruction: Can raw meat be stored on a shelf without a container?\n",
      "Generated: Store raw meat and fish in suitable containers helps prevent them from touching other foods or leaking fluids, which is essential for preventing cross-contamination and ensuring food safety.\n",
      "Expected: no\n",
      "--------------------------------------------------\n",
      "Instruction: Does PowerCool turn off automatically after a few hours?\n",
      "Generated: I apologize, but I am a refrigerator assistant and cannot help with medical advice.\n",
      "Expected: yes\n",
      "--------------------------------------------------\n",
      "Instruction: Can old hose-sets be reused for the water connection?\n",
      "Generated: The appliance ought to be limited to potable water supplies to maintain water safety and quality within the device.\n",
      "Expected: no\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pair in input_pairs:\n",
    "    instruction = pair[\"instruction\"]\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Expected:\", pair[\"response\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b425741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user placed raw meat in a container and put it in the fridge so _ would not contaminate other food. meat container\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/WinoGrande_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"instruction\": item[\"instruction\"], \"response\": item[\"response\"]} for item in data]\n",
    "print(input_pairs[0][\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac97a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: The user placed raw meat in a container and put it in the fridge so _ would not contaminate other food. meat container\n",
      "Generated: Store raw meat and fish in suitable containers to prevent their juices from dripping onto other foods and to maintain safe, contamination-free storage.\n",
      "Expected: meat\n",
      "--------------------------------------------------\n",
      "Instruction: The child opened the fridge door, although _ was heavy. child door\n",
      "Generated: Do not hang on the refrigerator door, as this may lead to benzene or operational problems, ensuring the door doesn't pinch the cable.\n",
      "Expected: door\n",
      "--------------------------------------------------\n",
      "Instruction: The technician checked the compressor because _ was noisy. technician compressor\n",
      "Generated: I apologize, but I am a refrigerator assistant and cannot help with historical facts.\n",
      "Expected: compressor\n",
      "--------------------------------------------------\n",
      "Instruction: The user put a bottle in the fridge door because _ was tall. bottle door\n",
      "Generated: To avoid creating a variety of package sizes. This adjustability gives users greater flexibility to organize and utilize the refrigerator’s storage space for assorted food container sizes and types.\n",
      "Expected: bottle\n",
      "--------------------------------------------------\n",
      "Instruction: The mother gave the manual to the child so _ would know how to use the fridge. mother child\n",
      "Generated: To maximize safety instructions can be kept refrigerated, it must be turned off, defrosted, cleaned, and left open to dry. This prevents mold growth and maintains the appliance's cleanliness and functionality.\n",
      "Expected: child\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for pair in input_pairs:\n",
    "    instruction = pair[\"instruction\"]\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(\"Expected:\", pair[\"response\"])\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fede5cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словник завантажено. Розмір: 22695 токенів\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# --- Завантаження словника ---\n",
    "with open(f\"../models/TinyStories/vocab_{numb}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "id2new = {int(k): int(v) for k, v in vocab_data[\"id2new\"].items()}\n",
    "rev_map = {int(k): int(v) for k, v in vocab_data[\"rev_map\"].items()}\n",
    "\n",
    "print(f\"Словник завантажено. Розмір: {len(id2new)} токенів\")\n",
    "def remap_ids(ids):\n",
    "    return [id2new[i] for i in ids if i in id2new]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33202567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Шукаю окремі битові символи...\n",
      "  Знайдено битовий символ: new_id=1, orig_id=1, '\"' -> '\"'\n",
      "  Знайдено токен з битовими символами: new_id=366, orig_id=366, ' \"' -> ' \"'\n",
      "  Знайдено токен з битовими символами: new_id=525, orig_id=526, '.\"' -> '.\"'\n",
      "  Знайдено токен з битовими символами: new_id=552, orig_id=553, ',\"' -> ',\"'\n",
      "  Знайдено битовий символ: new_id=938, orig_id=960, '—' -> '-'\n",
      "  Знайдено токен з битовими символами: new_id=1241, orig_id=1298, '\":' -> '\":'\n",
      "  Знайдено токен з битовими символами: new_id=1502, orig_id=1600, '\",' -> '\",'\n",
      "  Знайдено токен з битовими символами: new_id=1595, orig_id=1701, '?\"' -> '?\"'\n",
      "  Знайдено токен з битовими символами: new_id=1760, orig_id=1911, '\".' -> '\".'\n",
      "  Знайдено токен з битовими символами: new_id=2217, orig_id=2474, '!\"' -> '!\"'\n",
      "  Знайдено токен з битовими символами: new_id=6357, orig_id=7879, '\\\"' -> '\\\"'\n",
      "  Знайдено битовий символ: new_id=6549, orig_id=8151, '™' -> '''\n",
      "  Знайдено токен з битовими символами: new_id=7327, orig_id=9313, '...\"' -> '...\"'\n",
      "  Знайдено токен з битовими символами: new_id=8484, orig_id=11097, ':\"' -> ':\"'\n",
      "  Знайдено токен з битовими символами: new_id=8741, orig_id=11496, '.'\"' -> '.'\"'\n",
      "  Знайдено токен з битовими символами: new_id=9620, orig_id=12878, ' \"[' -> ' \"['\n",
      "  Знайдено токен з битовими символами: new_id=10008, orig_id=13538, ' \"\"' -> ' \"\"'\n",
      "  Знайдено токен з битовими символами: new_id=10279, orig_id=13984, '\"?' -> '\"?'\n",
      "  Знайдено токен з битовими символами: new_id=11486, orig_id=16078, ','\"' -> ','\"'\n",
      "  Знайдено токен з битовими символами: new_id=12499, orig_id=17971, ' \"$' -> ' \"$'\n",
      "  Знайдено токен з битовими символами: new_id=13468, orig_id=19990, ' \\\"' -> ' \\\"'\n",
      "  Знайдено токен з битовими символами: new_id=14096, orig_id=21215, '-\"' -> '-\"'\n",
      "  Знайдено токен з битовими символами: new_id=14529, orig_id=22135, ' .\"' -> ' .\"'\n",
      "  Знайдено битовий символ: new_id=14900, orig_id=22940, 'â' -> '\"'\n",
      "  Знайдено токен з битовими символами: new_id=15415, orig_id=24018, ' \"'' -> ' \"''\n",
      "  Знайдено токен з битовими символами: new_id=16399, orig_id=26214, '\"...' -> '\"...'\n",
      "  Знайдено битовий символ: new_id=16472, orig_id=26391, '€' -> ''\n",
      "  Знайдено токен з битовими символами: new_id=16648, orig_id=26793, '\"-' -> '\"-'\n",
      "  Знайдено токен з битовими символами: new_id=16730, orig_id=26989, '?'\"' -> '?'\"'\n",
      "  Знайдено токен з битовими символами: new_id=16758, orig_id=27071, ' \".' -> ' \".'\n",
      "  Знайдено токен з битовими символами: new_id=16920, orig_id=27444, ' \"-' -> ' \"-'\n",
      "  Знайдено токен з битовими символами: new_id=17100, orig_id=27896, ' \"...' -> ' \"...'\n",
      "  Знайдено токен з битовими символами: new_id=17771, orig_id=29653, ''\"' -> ''\"'\n",
      "  Знайдено токен з битовими символами: new_id=18103, orig_id=30543, '\"'' -> '\"''\n",
      "  Знайдено токен з битовими символами: new_id=18213, orig_id=30823, '?!\"' -> '?!\"'\n",
      "  Знайдено токен з битовими символами: new_id=18215, orig_id=30827, ''.\"' -> ''.\"'\n",
      "  Знайдено токен з битовими символами: new_id=18711, orig_id=32203, '.\"\"' -> '.\"\"'\n",
      "  Знайдено токен з битовими символами: new_id=19066, orig_id=33172, ' \",' -> ' \",'\n",
      "  Знайдено токен з битовими символами: new_id=19114, orig_id=33283, '.\",' -> '.\",'\n",
      "  Знайдено токен з битовими символами: new_id=19894, orig_id=35379, '?\",' -> '?\",'\n",
      "  Знайдено токен з битовими символами: new_id=20018, orig_id=35713, ' ...\"' -> ' ...\"'\n",
      "  Знайдено токен з битовими символами: new_id=20516, orig_id=37160, '!!\"' -> '!!\"'\n",
      "  Знайдено токен з битовими символами: new_id=21546, orig_id=40264, '',\"' -> '',\"'\n",
      "  Знайдено токен з битовими символами: new_id=21622, orig_id=40484, '\"!' -> '\"!'\n",
      "  Знайдено токен з битовими символами: new_id=21691, orig_id=40754, '!\",' -> '!\",'\n",
      "  Знайдено токен з битовими символами: new_id=22303, orig_id=42720, '!?\"' -> '!?\"'\n",
      "  Знайдено токен з битовими символами: new_id=22377, orig_id=42911, ' ,\"' -> ' ,\"'\n",
      "  Знайдено токен з битовими символами: new_id=22599, orig_id=43634, '?\".' -> '?\".'\n",
      "  Знайдено токен з битовими символами: new_id=23909, orig_id=48220, '!\".' -> '!\".'\n",
      "  Знайдено токен з битовими символами: new_id=24226, orig_id=49296, '!'\"' -> '!'\"'\n",
      "\n",
      "Знайдено 50 проблемних токенів\n",
      "  Замінено: '\"' -> '\"' (orig_id: 1 -> 1)\n",
      "  Замінено: '—' -> '-' (orig_id: 960 -> 12)\n",
      "  Замінено: '™' -> ''' (orig_id: 8151 -> 6)\n",
      "  Замінено: 'â' -> '\"' (orig_id: 22940 -> 1)\n",
      "  Замінено: '€' -> ' ' (orig_id: 26391 -> 220)\n",
      "\n",
      "Виправлений словник збережено в ../models/TinyStories/vocab_24497_fixed.json\n"
     ]
    }
   ],
   "source": [
    "def fix_vocab_individual_broken_chars(vocab_file, output_file):\n",
    "    \"\"\"Виправляє окремі битові Unicode символи у словнику\"\"\"\n",
    "    \n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab_data = json.load(f)\n",
    "    \n",
    "    rev_map = {int(k): int(v) for k, v in vocab_data[\"rev_map\"].items()}\n",
    "    \n",
    "    # Список окремих битових символів для заміни\n",
    "    broken_chars = {\n",
    "        'â': '\"',  # або інший відповідний символ\n",
    "        '€': '',   # прибрати цей символ, бо він частина битової послідовності\n",
    "        '™': \"'\",  # якщо є\n",
    "        '\"': '\"',  # якщо є битові лапки\n",
    "        '\"': '\"',  # якщо є битові лапки\n",
    "        '—': '-',  # якщо є битове тире\n",
    "    }\n",
    "    \n",
    "    print(\"Шукаю окремі битові символи...\")\n",
    "    broken_tokens = {}\n",
    "    \n",
    "    for new_id, orig_id in rev_map.items():\n",
    "        try:\n",
    "            token_text = enc.decode([orig_id])\n",
    "            \n",
    "            # Перевіряємо чи токен складається ТІЛЬКИ з одного битового символу\n",
    "            if len(token_text) == 1 and token_text in broken_chars:\n",
    "                replacement_char = broken_chars[token_text]\n",
    "                print(f\"  Знайдено битовий символ: new_id={new_id}, orig_id={orig_id}, '{token_text}' -> '{replacement_char}'\")\n",
    "                broken_tokens[new_id] = (orig_id, token_text, replacement_char)\n",
    "                \n",
    "            # Також перевіряємо токени що МІСТЯТЬ битові символи\n",
    "            elif any(char in token_text for char in broken_chars):\n",
    "                cleaned_text = token_text\n",
    "                for broken_char, replacement in broken_chars.items():\n",
    "                    cleaned_text = cleaned_text.replace(broken_char, replacement)\n",
    "                print(f\"  Знайдено токен з битовими символами: new_id={new_id}, orig_id={orig_id}, '{token_text}' -> '{cleaned_text}'\")\n",
    "                broken_tokens[new_id] = (orig_id, token_text, cleaned_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nЗнайдено {len(broken_tokens)} проблемних токенів\")\n",
    "    \n",
    "    # Створюємо виправлений словник\n",
    "    fixed_rev_map = rev_map.copy()\n",
    "    \n",
    "    for new_id, (old_orig_id, broken_text, clean_text) in broken_tokens.items():\n",
    "        if clean_text == '':\n",
    "            # Якщо символ треба видалити, замінимо на пробіл\n",
    "            clean_text = ' '\n",
    "            \n",
    "        # Знайдемо orig_id для правильного символу\n",
    "        try:\n",
    "            if len(clean_text) == 1:\n",
    "                replacement_ids = enc.encode_ordinary(clean_text)\n",
    "                if replacement_ids:\n",
    "                    replacement_orig_id = replacement_ids[0]\n",
    "                    fixed_rev_map[new_id] = replacement_orig_id\n",
    "                    print(f\"  Замінено: '{broken_text}' -> '{clean_text}' (orig_id: {old_orig_id} -> {replacement_orig_id})\")\n",
    "        except:\n",
    "            print(f\"  Не вдалося замінити: '{broken_text}'\")\n",
    "    \n",
    "    # Створимо виправлений id2new\n",
    "    fixed_id2new = {v: k for k, v in fixed_rev_map.items()}\n",
    "    \n",
    "    # Збережемо\n",
    "    fixed_vocab_data = {\n",
    "        \"id2new\": {str(k): str(v) for k, v in fixed_id2new.items()},\n",
    "        \"rev_map\": {str(k): str(v) for k, v in fixed_rev_map.items()}\n",
    "    }\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(fixed_vocab_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nВиправлений словник збережено в {output_file}\")\n",
    "    return fixed_rev_map\n",
    "\n",
    "# Запуск\n",
    "fixed_rev_map = fix_vocab_individual_broken_chars(\n",
    "    f\"../models/TinyStories/vocab_{numb}.json\",\n",
    "    f\"../models/TinyStories/vocab_{numb}_fixed.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7275bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a good girl who loved hugs. She hugged everyone and everything she could. One day, she went to the park\n",
      "Once upon a time, there was a little boy named Tim. He liked to run and play all day. One day, Tim saw a big orange ball\n",
      "Once upon a time, there was a good man. He lived in a big house with his dog. One day, the man heard a loud\n",
      "Once upon a time, there was a happy owl. The owl lived in a big tree. The tree was in a pretty forest. The owl liked to play\n",
      "Once upon a time, there was a little girl named Lily. She had a pretty dress that she loved to wear. One day, Lily went to\n",
      "Once upon a time, there was a little boy named Tom. He loved to play all day. One day, he found\n",
      "Once there was a smart girl who wanted to win. Every day she worked hard, but never seemed to win. One day she decided to\n",
      "Once upon a time, there was a toy named Tim. Tim was a busy toy. He lived in a big toy box with\n",
      "Once upon a time, there was a funny little boy named Tim. He loved to play with his red ball. One day, he saw a big tree with\n",
      "Once upon a time, there was a little girl named Lily. She loved to run and play outside. One day, she went outside to play and realized she forgot\n"
     ]
    }
   ],
   "source": [
    "# PARTIAL dictionary\n",
    "results = []\n",
    "for inputs in input_pairs:\n",
    "    print(inputs[\"text\"])\n",
    "    sentence = inputs[\"text\"]\n",
    "    ids = enc.encode_ordinary(sentence)\n",
    "    new_ids = remap_ids(ids)\n",
    "    context = torch.tensor([new_ids], dtype=torch.long, device=device)\n",
    "    y = model.generate(context, 200)\n",
    "    orig_ids = [rev_map[i] for i in y.squeeze().tolist()]\n",
    "    output = enc.decode(orig_ids)\n",
    "    results.append({\n",
    "        \"input_text\": sentence,\n",
    "        \"generated_story\": output\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"model_40mb_result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f37dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 144 mb\n",
    "# 1 Grammar: 5/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5)\n",
    "# 2 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 3 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 4 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 5 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 6 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 7 Grammar: 6/10, Creativity: 7/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 8 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 9 Grammar: 6/10, Creativity: 7/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 10 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 67 mb\n",
    "# 1 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 2 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 3 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 4 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 5 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 6 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 7 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 8 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 9 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 10 Grammar: 3/10, Creativity: 6/10, Consistency: 2/10, Age group: B (4-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47.3 mb\n",
    "# 1 Grammar: 4/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5) Smaller\n",
    "# 2 Grammar: 5/10, Creativity: 5/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 3 Grammar: 4/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5)\n",
    "# 4 Grammar: 3/10, Creativity: 5/10, Consistency: 2/10, Age group: B (4-5)\n",
    "# 5 Grammar: 4/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 6 Grammar: 3/10, Creativity: 5/10, Consistency: 2/10, Age group: B (4-5) Token errors - Error with vocab\n",
    "# 7 Grammar: 4/10, Creativity: 5/10, Consistency: 3/10, Age group: B (4-5) Smaller\n",
    "# 8 Grammar: 4/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 9 Grammar: 4/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 10 Grammar: 5/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5) Smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47.3 mb second/third try\n",
    "# 1 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: B (4-5) Smaller\n",
    "# 2 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 3 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 4 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 5 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) \n",
    "# 6 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Token errors (fixed in 3 try)\n",
    "# 7 Grammar: 6/10, Creativity: 5/10, Consistency: 5/10, Age group: B (4-5) Smaller\n",
    "# 8 Grammar: 4/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) \n",
    "# 9 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 10 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: B (4-5) Smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb26fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44.8 mb\n",
    "# 1 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 2 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 3 Grammar: 4/10, Creativity: 5/10, Consistency: 3/10, Age group: B (4-5) Very Smaller\n",
    "# 4 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 5 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 6 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 7 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 8 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 9 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 10 Grammar: 4/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5) Smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945be132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43.1 mb\n",
    "# 1 Grammar: 5/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Very Smaller\n",
    "# 2 Grammar: 4/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5)\n",
    "# 3 Grammar: 5/10, Creativity: 5/10, Consistency: 4/10, Age group: C (6-7) Medium Smaller\n",
    "# 4 Grammar: 5/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 5 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: C (6-7) Smaller\n",
    "# 6 Grammar: 4/10, Creativity: 5/10, Consistency: 3/10, Age group: B (4-5) Medium Smaller\n",
    "# 7 Grammar: 2/10, Creativity: 4/10, Consistency: 2/10, Age group: A (3 or under) Smaller\n",
    "# 8 Grammar: 4/10, Creativity: 5/10, Consistency: 4/10, Age group: B (4-5) Very Smaller\n",
    "# 9 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Smaller\n",
    "# 10 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b947248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42.1 mb (Second try fixed vocab)\n",
    "# 1 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Medium Smaller\n",
    "# 2 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Medium Smaller\n",
    "# 3 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Second try Smaller\n",
    "# 4 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Medium Smaller\n",
    "# 5 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 6 Grammar: 7/10, Creativity: 7/10, Consistency: 6/10, Age group: C (6-7) Smaller\n",
    "# 7 Grammar: 7/10, Creativity: 7/10, Consistency: 6/10, Age group: C (6-7) Medium Smaller\n",
    "\n",
    "# 8 Grammar: 3/10, Creativity: 5/10, Consistency: 3/10, Age group: B (4-5) Smaller - Another GPTChat, another answer...\n",
    "# 8 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Another GPTChat, another answer...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d774f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
