{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a9f843",
   "metadata": {
    "executionInfo": {
     "elapsed": 9458,
     "status": "ok",
     "timestamp": 1753204059134,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "f0a9f843"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oTIJaMb46vx8",
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1753204078270,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "oTIJaMb46vx8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "FPK7Np-0Sc-h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1753204081690,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "FPK7Np-0Sc-h",
    "outputId": "cc50a54e-5c65-4a46-bb7c-bde5bf5736f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "U2fCyrEU6XCW",
   "metadata": {
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1753204083841,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "U2fCyrEU6XCW"
   },
   "outputs": [],
   "source": [
    "numb = 50257\n",
    "# 50257\n",
    "config = GPTConfig(\n",
    "    vocab_size=numb,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cab9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_model_load(model, path, numb):\n",
    "    try:\n",
    "        # Завантажте на CPU спочатку\n",
    "        print(\"Завантажую на CPU...\")\n",
    "        # checkpoint = torch.load(f\"{path}/gpt_1.3_new_gpt_50ep.pt\", map_location='cpu')\n",
    "        # checkpoint = torch.load(f\"{path}/TinyStories_{numb}.pt\", map_location='cpu')\n",
    "        checkpoint = torch.load(f\"{path}/114mb_gpt20b-oss.pt\", map_location='cpu')\n",
    "        \n",
    "        # Завантажте state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State dict завантажено\")\n",
    "        \n",
    "        # Очистіть checkpoint з пам'яті\n",
    "        del checkpoint\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Поступово перенесіть на GPU\n",
    "        print(\"Переношу на GPU...\")\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Модель успішно завантажена на GPU\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Помилка завантаження: {e}\")\n",
    "        print(\"Залишаю модель на CPU\")\n",
    "        model.eval()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nz4Txftb6esb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1753204155915,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "nz4Txftb6esb",
    "outputId": "3a4baa7c-99d6-4d2f-ba98-4abcf2b757ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завантажую на CPU...\n",
      "State dict завантажено\n",
      "Переношу на GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matsk\\AppData\\Local\\Temp\\ipykernel_7608\\1063699071.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{path}/114mb_gpt20b-oss.pt\", map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успішно завантажена на GPU\n",
      "Модель на пристрої: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = safe_model_load(model, \"../models/\", numb)\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Модель на пристрої: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8357ed5d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1596,
     "status": "ok",
     "timestamp": 1753204162535,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "8357ed5d"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "block_size = config.block_size  # =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d560a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does this manual contain?\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/test_dataset_40.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"text\": item[\"text\"]} for item in data]\n",
    "print(input_pairs[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf717e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "import random as _random\n",
    "_random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99418466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(instruction, max_new_tokens=100, temperature=0.6, top_k=40):\n",
    "    prompt = f\"question: {instruction}\\nanswer:\"\n",
    "    input_ids = enc.encode_ordinary(prompt)\n",
    "    input_ids = input_ids[:config.block_size]\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long)[None].to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            if input_tensor.shape[1] > config.block_size:\n",
    "                input_tensor = input_tensor[:, -config.block_size:]\n",
    "\n",
    "            logits, _ = model(input_tensor)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                values, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < values[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_tensor = torch.cat([input_tensor, next_token], dim=1)\n",
    "\n",
    "            # Зупиняємо генерацію, якщо згенеровано токен <|END|> (50256)\n",
    "            if next_token.item() == 50256:\n",
    "                break\n",
    "\n",
    "    output_tokens = input_tensor[0].tolist()\n",
    "    generated = enc.decode(output_tokens[len(input_ids):])\n",
    "    return generated.strip().replace(\"<|END|><|endoftext|>\", \"\")  # Видаляємо <|END|> із виводу\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb25c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Інструкція: What does this manual contain?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Why should I read the manual?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What does the “Warning” icon mean?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What water pressure should the refrigerator have?\n",
      "Відповідь: output<|END|END|><|endoftext|>\n",
      "--------------------------------------------------\n",
      "Інструкція: How can I replace the LED lamps?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What should I do to avoid contamination of food?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: How many people are required to transport the refrigerator?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: How many fingers are on a left hand?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What is the temperature outside?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What does water taste like?\n",
      "Відповідь: output<|END|END|><|endoftext|>\n",
      "--------------------------------------------------\n",
      "Інструкція: Can I touch the tempered glass with my hands?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Should I apply force to open the door if it’s frozen?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Can I eat frozen food immediately after removing it from the freezer?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Can I use vinegar to clean the refrigerator?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Is the climate class shown on the rating plate?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Is the optimal refrigerator temperature 5 degrees Celsius?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Is the optimal freezer temperature -20 degrees Celsius?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Can I drink tap water?\n",
      "Відповідь: output<|END|END|><|endoftext|>\n",
      "--------------------------------------------------\n",
      "Інструкція: Can I eat candies?\n",
      "Відповідь: output<|END|END|><|endoftext|>\n",
      "--------------------------------------------------\n",
      "Інструкція: Can cotton candy be salty?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Does Power Cool speed up the cooling process or the freezing process?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Does Power Freeze speed up the cooling process or the freezing process?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: In Power Freeze mode, does the freezer run at full speed for 40 hours or 45?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: In Power Cool mode, does the refrigerator run at full speed for 2 hours or 3?\n",
      "Відповідь: output<|><|endoftext|>\n",
      "--------------------------------------------------\n",
      "Інструкція: Can I put food on the shelves or in the specialized compartment?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Is the recommended storage time for milk in the refrigerator 1 week or 2 weeks?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Is the recommended storage time for butter in the freezer 1 week or 2 weeks?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: Can I eat fried food and stay healthy, or should I eat only boiled food?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: If I don’t want to go to school, can I stay home or I need to go?\n",
      "Відповідь: output<|><|endoftext|>\n",
      "--------------------------------------------------\n",
      "Інструкція: Can I sell a broken phone, or does it have to be new?\n",
      "Відповідь: output<|END|END|><|endoftext|>\n",
      "--------------------------------------------------\n",
      "Інструкція: What happens if I store butter in the refrigerator for three weeks?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What happens if I store sausages in the freezer for three weeks?\n",
      "Відповідь: output<|END|END|><|endoftext|>\n",
      "--------------------------------------------------\n",
      "Інструкція: What happens if I store fish in the freezer for seven months?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What should I do if my refrigerator is broken?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What should I do if the refrigerator leaks?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What should I do if I don’t know how to turn on the refrigerator?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What should I do if the light bulb in the refrigerator burns out?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What should I do if my tooth hurts?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What should I do if I don’t want to study?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n",
      "Інструкція: What happens if I don’t water my flowers tomorrow?\n",
      "Відповідь: output\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# FULL dictionary\n",
    "results = []\n",
    "for inputs in input_pairs:\n",
    "    instruction = inputs[\"text\"]\n",
    "    response = generate_response(instruction)\n",
    "    results.append({\n",
    "        \"instruction\": instruction,\n",
    "        \"response\": response\n",
    "    })\n",
    "    print(\"Інструкція:\", instruction)\n",
    "    print(\"Відповідь:\", response)\n",
    "    print(\"-\" * 50)\n",
    "# print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35e127ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Збереження результатів у файл\n",
    "with open(\"fridge_gpt3.1_openchat_responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Збереження результатів у txt файл\n",
    "with open(\"fridge_gpt3.1_openchat_responses.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in results:\n",
    "        f.write(f\"Інструкція: {item['instruction']}\\n\")\n",
    "        f.write(f\"Відповідь: {item['response']}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
