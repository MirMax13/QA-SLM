{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a57ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\All\\Projects\\QA-SLM\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d361036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1e54de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  30 M    \n",
      "fwd MACs:                                                               1.38 GMACs\n",
      "fwd FLOPs:                                                              2.76 GFLOPS\n",
      "fwd+bwd MACs:                                                           4.13 GMACs\n",
      "fwd+bwd FLOPs:                                                          8.28 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "GPTWrapper(\n",
      "  30 M = 100% Params, 1.38 GMACs = 100% MACs, 2.76 GFLOPS = 100% FLOPs\n",
      "  (model): GPT(\n",
      "    30 M = 100% Params, 1.38 GMACs = 100% MACs, 2.76 GFLOPS = 100% FLOPs\n",
      "    (transformer): ModuleDict(\n",
      "      30 M = 100% Params, 1.36 GMACs = 98.6% MACs, 2.72 GFLOPS = 98.6% FLOPs\n",
      "      (wte): Embedding(19.3 M = 64.34% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 50257, 384)\n",
      "      (wpe): Embedding(49.15 K = 0.16% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 128, 384)\n",
      "      (drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-5): 6 x Block(\n",
      "          1.77 M = 5.92% Params, 226.49 MMACs = 16.43% MACs, 453.67 MFLOPS = 16.43% FLOPs\n",
      "          (ln1): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 245.76 KFLOPS = 0.01% FLOPs)\n",
      "          (attn): CausalSelfAttention(\n",
      "            591.36 K = 1.97% Params, 75.5 MMACs = 5.48% MACs, 150.99 MFLOPS = 5.47% FLOPs\n",
      "            (c_attn): Linear(443.52 K = 1.48% Params, 56.62 MMACs = 4.11% MACs, 113.25 MFLOPS = 4.1% FLOPs, in_features=384, out_features=1152, bias=True)\n",
      "            (c_proj): Linear(147.84 K = 0.49% Params, 18.87 MMACs = 1.37% MACs, 37.75 MFLOPS = 1.37% FLOPs, in_features=384, out_features=384, bias=True)\n",
      "            (attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln2): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 245.76 KFLOPS = 0.01% FLOPs)\n",
      "          (mlp): MLP(\n",
      "            1.18 M = 3.94% Params, 150.99 MMACs = 10.96% MACs, 302.19 MFLOPS = 10.95% FLOPs\n",
      "            (c_fc): Linear(591.36 K = 1.97% Params, 75.5 MMACs = 5.48% MACs, 150.99 MFLOPS = 5.47% FLOPs, in_features=384, out_features=1536, bias=True)\n",
      "            (gelu): GELU(0 = 0% Params, 0 MACs = 0% MACs, 196.61 KFLOPS = 0.01% FLOPs, approximate='none')\n",
      "            (c_proj): Linear(590.21 K = 1.97% Params, 75.5 MMACs = 5.48% MACs, 150.99 MFLOPS = 5.47% FLOPs, in_features=1536, out_features=384, bias=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm(768 = 0% Params, 0 MACs = 0% MACs, 245.76 KFLOPS = 0.01% FLOPs)\n",
      "    )\n",
      "    (lm_head): Linear(19.3 M = 64.34% Params, 19.3 MMACs = 1.4% MACs, 38.6 MFLOPS = 1.4% FLOPs, in_features=384, out_features=50257, bias=False)\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n",
      "GPT FLOPs: 2.76 GFLOPS\n",
      "GPT MACs: 1.38 GMACs\n",
      "Params: 30.00 M\n"
     ]
    }
   ],
   "source": [
    "from calflops import calculate_flops\n",
    "import tiktoken\n",
    "\n",
    "# Обгортка для токенізатора\n",
    "class GPTTokenizerWrapper:\n",
    "    def __init__(self, enc, block_size):\n",
    "        self.enc = enc\n",
    "        self.block_size = block_size\n",
    "        self.pad_token_id = 0\n",
    "        self.cls_token_id = 50256\n",
    "        \n",
    "    def encode_plus(self, text, **kwargs):\n",
    "        tokens = self.enc.encode_ordinary(text)\n",
    "        if len(tokens) > self.block_size - 1:\n",
    "            tokens = tokens[:self.block_size - 1] + [self.cls_token_id]\n",
    "        else:\n",
    "            tokens = tokens + [self.cls_token_id] + [self.pad_token_id] * (self.block_size - len(tokens) - 1)\n",
    "        return {\n",
    "            \"input_ids\": tokens,\n",
    "            \"attention_mask\": [1] * len(tokens)\n",
    "        }\n",
    "\n",
    "# Обгортка для моделі\n",
    "class GPTWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids)\n",
    "\n",
    "# Ініціалізація\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer = GPTTokenizerWrapper(enc, config.block_size)\n",
    "wrapped_model = GPTWrapper(model)\n",
    "\n",
    "# Підрахунок FLOPs\n",
    "flops, macs, params = calculate_flops(\n",
    "    model=wrapped_model,\n",
    "    input_shape=(1, config.block_size),\n",
    "    transformer_tokenizer=tokenizer,\n",
    "    output_as_string=False  # Це ключовий параметр!\n",
    ")\n",
    "\n",
    "# Конвертація результатів\n",
    "flops_num = float(flops.split()[0]) * 1e9 if isinstance(flops, str) else flops\n",
    "macs_num = float(macs.split()[0]) * 1e9 if isinstance(macs, str) else macs\n",
    "params_num = float(params.split()[0]) * 1e6 if isinstance(params, str) else params\n",
    "\n",
    "print(f\"GPT FLOPs: {flops_num/1e9:.2f} GFLOPS\")\n",
    "print(f\"GPT MACs: {macs_num/1e9:.2f} GMACs\") \n",
    "print(f\"Params: {params_num/1e6:.2f} M\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
