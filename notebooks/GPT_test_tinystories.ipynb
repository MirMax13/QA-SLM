{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a9f843",
   "metadata": {
    "executionInfo": {
     "elapsed": 9458,
     "status": "ok",
     "timestamp": 1753204059134,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "f0a9f843"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\All\\Projects\\QA-SLM\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oTIJaMb46vx8",
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1753204078270,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "oTIJaMb46vx8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "FPK7Np-0Sc-h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1753204081690,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "FPK7Np-0Sc-h",
    "outputId": "cc50a54e-5c65-4a46-bb7c-bde5bf5736f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "U2fCyrEU6XCW",
   "metadata": {
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1753204083841,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "U2fCyrEU6XCW"
   },
   "outputs": [],
   "source": [
    "numb = 24497\n",
    "# 50257\n",
    "config = GPTConfig(\n",
    "    vocab_size=numb,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",
    "    n_head=4,\n",
    "    n_embd=256,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cab9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_model_load(model, path, numb):\n",
    "    try:\n",
    "        # Завантажте на CPU спочатку\n",
    "        print(\"Завантажую на CPU...\")\n",
    "        # checkpoint = torch.load(f\"{path}/gpt_1.3_new_gpt_50ep.pt\", map_location='cpu')\n",
    "        checkpoint = torch.load(f\"{path}/TinyStories_{numb}.pt\", map_location='cpu')\n",
    "        # checkpoint = torch.load(f\"{path}/114mb_20000.pt\", map_location='cpu')\n",
    "        \n",
    "        # Завантажте state dict\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"State dict завантажено\")\n",
    "        \n",
    "        # Очистіть checkpoint з пам'яті\n",
    "        del checkpoint\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Поступово перенесіть на GPU\n",
    "        print(\"Переношу на GPU...\")\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        \n",
    "        print(\"Модель успішно завантажена на GPU\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Помилка завантаження: {e}\")\n",
    "        print(\"Залишаю модель на CPU\")\n",
    "        model.eval()\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nz4Txftb6esb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1753204155915,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "nz4Txftb6esb",
    "outputId": "3a4baa7c-99d6-4d2f-ba98-4abcf2b757ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Завантажую на CPU...\n",
      "State dict завантажено\n",
      "Переношу на GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matsk\\AppData\\Local\\Temp\\ipykernel_6460\\966027571.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f\"{path}/TinyStories_{numb}.pt\", map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успішно завантажена на GPU\n",
      "Модель на пристрої: cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = safe_model_load(model, \"../models/TinyStories\", numb)\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Модель на пристрої: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8357ed5d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1596,
     "status": "ok",
     "timestamp": 1753204162535,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "8357ed5d"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "block_size = config.block_size  # =128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d560a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a good girl who loved hugs. She hugged everyone and everything she could. One day, she went to the park\n"
     ]
    }
   ],
   "source": [
    "with open(\"../eval_selection/val_eval_samples_popular_input.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "input_pairs = [{\"text\": item[\"text\"]} for item in data]\n",
    "print(input_pairs[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf717e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 12345\n",
    "import random as _random\n",
    "_random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bb25c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a good girl who loved hugs. She hugged everyone and everything she could. One day, she went to the park\n",
      "Once upon a time, there was a little boy named Tim. He liked to run and play all day. One day, Tim saw a big orange ball\n",
      "Once upon a time, there was a good man. He lived in a big house with his dog. One day, the man heard a loud\n",
      "Once upon a time, there was a happy owl. The owl lived in a big tree. The tree was in a pretty forest. The owl liked to play\n",
      "Once upon a time, there was a little girl named Lily. She had a pretty dress that she loved to wear. One day, Lily went to\n",
      "Once upon a time, there was a little boy named Tom. He loved to play all day. One day, he found\n",
      "Once there was a smart girl who wanted to win. Every day she worked hard, but never seemed to win. One day she decided to\n",
      "Once upon a time, there was a toy named Tim. Tim was a busy toy. He lived in a big toy box with\n",
      "Once upon a time, there was a funny little boy named Tim. He loved to play with his red ball. One day, he saw a big tree with\n",
      "Once upon a time, there was a little girl named Lily. She loved to run and play outside. One day, she went outside to play and realized she forgot\n"
     ]
    }
   ],
   "source": [
    "# FULL dictionary\n",
    "results = []\n",
    "for inputs in input_pairs:\n",
    "    print(inputs[\"text\"])\n",
    "    sentence = inputs[\"text\"]\n",
    "    context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0).to(device))\n",
    "    y = model.generate(context, 200)\n",
    "    output = enc.decode(y.squeeze().tolist())\n",
    "    results.append({\n",
    "        \"input_text\": sentence,\n",
    "        \"generated_story\": output\n",
    "    })\n",
    "# print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fede5cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словник завантажено. Розмір: 24497 токенів\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# --- Завантаження словника ---\n",
    "with open(f\"../models/TinyStories/vocab_{numb}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_data = json.load(f)\n",
    "\n",
    "id2new = {int(k): int(v) for k, v in vocab_data[\"id2new\"].items()}\n",
    "rev_map = {int(k): int(v) for k, v in vocab_data[\"rev_map\"].items()}\n",
    "\n",
    "print(f\"Словник завантажено. Розмір: {len(id2new)} токенів\")\n",
    "def remap_ids(ids):\n",
    "    return [id2new[i] for i in ids if i in id2new]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33202567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Шукаю окремі битові символи...\n",
      "  Знайдено битовий символ: new_id=1, orig_id=1, '\"' -> '\"'\n",
      "  Знайдено токен з битовими символами: new_id=366, orig_id=366, ' \"' -> ' \"'\n",
      "  Знайдено токен з битовими символами: new_id=525, orig_id=526, '.\"' -> '.\"'\n",
      "  Знайдено токен з битовими символами: new_id=552, orig_id=553, ',\"' -> ',\"'\n",
      "  Знайдено битовий символ: new_id=938, orig_id=960, '—' -> '-'\n",
      "  Знайдено токен з битовими символами: new_id=1241, orig_id=1298, '\":' -> '\":'\n",
      "  Знайдено токен з битовими символами: new_id=1502, orig_id=1600, '\",' -> '\",'\n",
      "  Знайдено токен з битовими символами: new_id=1595, orig_id=1701, '?\"' -> '?\"'\n",
      "  Знайдено токен з битовими символами: new_id=1760, orig_id=1911, '\".' -> '\".'\n",
      "  Знайдено токен з битовими символами: new_id=2217, orig_id=2474, '!\"' -> '!\"'\n",
      "  Знайдено токен з битовими символами: new_id=6357, orig_id=7879, '\\\"' -> '\\\"'\n",
      "  Знайдено битовий символ: new_id=6549, orig_id=8151, '™' -> '''\n",
      "  Знайдено токен з битовими символами: new_id=7327, orig_id=9313, '...\"' -> '...\"'\n",
      "  Знайдено токен з битовими символами: new_id=8484, orig_id=11097, ':\"' -> ':\"'\n",
      "  Знайдено токен з битовими символами: new_id=8741, orig_id=11496, '.'\"' -> '.'\"'\n",
      "  Знайдено токен з битовими символами: new_id=9620, orig_id=12878, ' \"[' -> ' \"['\n",
      "  Знайдено токен з битовими символами: new_id=10008, orig_id=13538, ' \"\"' -> ' \"\"'\n",
      "  Знайдено токен з битовими символами: new_id=10279, orig_id=13984, '\"?' -> '\"?'\n",
      "  Знайдено токен з битовими символами: new_id=11486, orig_id=16078, ','\"' -> ','\"'\n",
      "  Знайдено токен з битовими символами: new_id=12499, orig_id=17971, ' \"$' -> ' \"$'\n",
      "  Знайдено токен з битовими символами: new_id=13468, orig_id=19990, ' \\\"' -> ' \\\"'\n",
      "  Знайдено токен з битовими символами: new_id=14096, orig_id=21215, '-\"' -> '-\"'\n",
      "  Знайдено токен з битовими символами: new_id=14529, orig_id=22135, ' .\"' -> ' .\"'\n",
      "  Знайдено битовий символ: new_id=14900, orig_id=22940, 'â' -> '\"'\n",
      "  Знайдено токен з битовими символами: new_id=15415, orig_id=24018, ' \"'' -> ' \"''\n",
      "  Знайдено токен з битовими символами: new_id=16399, orig_id=26214, '\"...' -> '\"...'\n",
      "  Знайдено битовий символ: new_id=16472, orig_id=26391, '€' -> ''\n",
      "  Знайдено токен з битовими символами: new_id=16648, orig_id=26793, '\"-' -> '\"-'\n",
      "  Знайдено токен з битовими символами: new_id=16730, orig_id=26989, '?'\"' -> '?'\"'\n",
      "  Знайдено токен з битовими символами: new_id=16758, orig_id=27071, ' \".' -> ' \".'\n",
      "  Знайдено токен з битовими символами: new_id=16920, orig_id=27444, ' \"-' -> ' \"-'\n",
      "  Знайдено токен з битовими символами: new_id=17100, orig_id=27896, ' \"...' -> ' \"...'\n",
      "  Знайдено токен з битовими символами: new_id=17771, orig_id=29653, ''\"' -> ''\"'\n",
      "  Знайдено токен з битовими символами: new_id=18103, orig_id=30543, '\"'' -> '\"''\n",
      "  Знайдено токен з битовими символами: new_id=18213, orig_id=30823, '?!\"' -> '?!\"'\n",
      "  Знайдено токен з битовими символами: new_id=18215, orig_id=30827, ''.\"' -> ''.\"'\n",
      "  Знайдено токен з битовими символами: new_id=18711, orig_id=32203, '.\"\"' -> '.\"\"'\n",
      "  Знайдено токен з битовими символами: new_id=19066, orig_id=33172, ' \",' -> ' \",'\n",
      "  Знайдено токен з битовими символами: new_id=19114, orig_id=33283, '.\",' -> '.\",'\n",
      "  Знайдено токен з битовими символами: new_id=19894, orig_id=35379, '?\",' -> '?\",'\n",
      "  Знайдено токен з битовими символами: new_id=20018, orig_id=35713, ' ...\"' -> ' ...\"'\n",
      "  Знайдено токен з битовими символами: new_id=20516, orig_id=37160, '!!\"' -> '!!\"'\n",
      "  Знайдено токен з битовими символами: new_id=21546, orig_id=40264, '',\"' -> '',\"'\n",
      "  Знайдено токен з битовими символами: new_id=21622, orig_id=40484, '\"!' -> '\"!'\n",
      "  Знайдено токен з битовими символами: new_id=21691, orig_id=40754, '!\",' -> '!\",'\n",
      "  Знайдено токен з битовими символами: new_id=22303, orig_id=42720, '!?\"' -> '!?\"'\n",
      "  Знайдено токен з битовими символами: new_id=22377, orig_id=42911, ' ,\"' -> ' ,\"'\n",
      "  Знайдено токен з битовими символами: new_id=22599, orig_id=43634, '?\".' -> '?\".'\n",
      "  Знайдено токен з битовими символами: new_id=23909, orig_id=48220, '!\".' -> '!\".'\n",
      "  Знайдено токен з битовими символами: new_id=24226, orig_id=49296, '!'\"' -> '!'\"'\n",
      "\n",
      "Знайдено 50 проблемних токенів\n",
      "  Замінено: '\"' -> '\"' (orig_id: 1 -> 1)\n",
      "  Замінено: '—' -> '-' (orig_id: 960 -> 12)\n",
      "  Замінено: '™' -> ''' (orig_id: 8151 -> 6)\n",
      "  Замінено: 'â' -> '\"' (orig_id: 22940 -> 1)\n",
      "  Замінено: '€' -> ' ' (orig_id: 26391 -> 220)\n",
      "\n",
      "Виправлений словник збережено в ../models/TinyStories/vocab_24497_fixed.json\n"
     ]
    }
   ],
   "source": [
    "def fix_vocab_individual_broken_chars(vocab_file, output_file):\n",
    "    \"\"\"Виправляє окремі битові Unicode символи у словнику\"\"\"\n",
    "    \n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab_data = json.load(f)\n",
    "    \n",
    "    rev_map = {int(k): int(v) for k, v in vocab_data[\"rev_map\"].items()}\n",
    "    \n",
    "    # Список окремих битових символів для заміни\n",
    "    broken_chars = {\n",
    "        'â': '\"',  # або інший відповідний символ\n",
    "        '€': '',   # прибрати цей символ, бо він частина битової послідовності\n",
    "        '™': \"'\",  # якщо є\n",
    "        '\"': '\"',  # якщо є битові лапки\n",
    "        '\"': '\"',  # якщо є битові лапки\n",
    "        '—': '-',  # якщо є битове тире\n",
    "    }\n",
    "    \n",
    "    print(\"Шукаю окремі битові символи...\")\n",
    "    broken_tokens = {}\n",
    "    \n",
    "    for new_id, orig_id in rev_map.items():\n",
    "        try:\n",
    "            token_text = enc.decode([orig_id])\n",
    "            \n",
    "            # Перевіряємо чи токен складається ТІЛЬКИ з одного битового символу\n",
    "            if len(token_text) == 1 and token_text in broken_chars:\n",
    "                replacement_char = broken_chars[token_text]\n",
    "                print(f\"  Знайдено битовий символ: new_id={new_id}, orig_id={orig_id}, '{token_text}' -> '{replacement_char}'\")\n",
    "                broken_tokens[new_id] = (orig_id, token_text, replacement_char)\n",
    "                \n",
    "            # Також перевіряємо токени що МІСТЯТЬ битові символи\n",
    "            elif any(char in token_text for char in broken_chars):\n",
    "                cleaned_text = token_text\n",
    "                for broken_char, replacement in broken_chars.items():\n",
    "                    cleaned_text = cleaned_text.replace(broken_char, replacement)\n",
    "                print(f\"  Знайдено токен з битовими символами: new_id={new_id}, orig_id={orig_id}, '{token_text}' -> '{cleaned_text}'\")\n",
    "                broken_tokens[new_id] = (orig_id, token_text, cleaned_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nЗнайдено {len(broken_tokens)} проблемних токенів\")\n",
    "    \n",
    "    # Створюємо виправлений словник\n",
    "    fixed_rev_map = rev_map.copy()\n",
    "    \n",
    "    for new_id, (old_orig_id, broken_text, clean_text) in broken_tokens.items():\n",
    "        if clean_text == '':\n",
    "            # Якщо символ треба видалити, замінимо на пробіл\n",
    "            clean_text = ' '\n",
    "            \n",
    "        # Знайдемо orig_id для правильного символу\n",
    "        try:\n",
    "            if len(clean_text) == 1:\n",
    "                replacement_ids = enc.encode_ordinary(clean_text)\n",
    "                if replacement_ids:\n",
    "                    replacement_orig_id = replacement_ids[0]\n",
    "                    fixed_rev_map[new_id] = replacement_orig_id\n",
    "                    print(f\"  Замінено: '{broken_text}' -> '{clean_text}' (orig_id: {old_orig_id} -> {replacement_orig_id})\")\n",
    "        except:\n",
    "            print(f\"  Не вдалося замінити: '{broken_text}'\")\n",
    "    \n",
    "    # Створимо виправлений id2new\n",
    "    fixed_id2new = {v: k for k, v in fixed_rev_map.items()}\n",
    "    \n",
    "    # Збережемо\n",
    "    fixed_vocab_data = {\n",
    "        \"id2new\": {str(k): str(v) for k, v in fixed_id2new.items()},\n",
    "        \"rev_map\": {str(k): str(v) for k, v in fixed_rev_map.items()}\n",
    "    }\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(fixed_vocab_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nВиправлений словник збережено в {output_file}\")\n",
    "    return fixed_rev_map\n",
    "\n",
    "# Запуск\n",
    "fixed_rev_map = fix_vocab_individual_broken_chars(\n",
    "    f\"../models/TinyStories/vocab_{numb}.json\",\n",
    "    f\"../models/TinyStories/vocab_{numb}_fixed.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7275bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a good girl who loved hugs. She hugged everyone and everything she could. One day, she went to the park\n",
      "Once upon a time, there was a little boy named Tim. He liked to run and play all day. One day, Tim saw a big orange ball\n",
      "Once upon a time, there was a good man. He lived in a big house with his dog. One day, the man heard a loud\n",
      "Once upon a time, there was a happy owl. The owl lived in a big tree. The tree was in a pretty forest. The owl liked to play\n",
      "Once upon a time, there was a little girl named Lily. She had a pretty dress that she loved to wear. One day, Lily went to\n",
      "Once upon a time, there was a little boy named Tom. He loved to play all day. One day, he found\n",
      "Once there was a smart girl who wanted to win. Every day she worked hard, but never seemed to win. One day she decided to\n",
      "Once upon a time, there was a toy named Tim. Tim was a busy toy. He lived in a big toy box with\n",
      "Once upon a time, there was a funny little boy named Tim. He loved to play with his red ball. One day, he saw a big tree with\n",
      "Once upon a time, there was a little girl named Lily. She loved to run and play outside. One day, she went outside to play and realized she forgot\n"
     ]
    }
   ],
   "source": [
    "# PARTIAL dictionary\n",
    "results = []\n",
    "for inputs in input_pairs:\n",
    "    print(inputs[\"text\"])\n",
    "    sentence = inputs[\"text\"]\n",
    "    ids = enc.encode_ordinary(sentence)\n",
    "    new_ids = remap_ids(ids)\n",
    "    context = torch.tensor([new_ids], dtype=torch.long, device=device)\n",
    "    y = model.generate(context, 200)\n",
    "    orig_ids = [rev_map[i] for i in y.squeeze().tolist()]\n",
    "    output = enc.decode(orig_ids)\n",
    "    results.append({\n",
    "        \"input_text\": sentence,\n",
    "        \"generated_story\": output\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61f8cb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a good man. He lived in a big house with his dog. One day, the man heard a loud sound coming from the road. It had a car that couldn\\'t lose it. He went to the dad and said, \"Hello! Why are you so nice?\"\\n\\nThe man replied, \"It\\'s okay. little family talked together all saying a time. It\\'s just talented ashles that it\\'s teach than Santa to be dangerous. What do we do and watch the animals stay away.\"\\n\\nThe man was happy, but the little girl had avoided the volcano. The squirrel was so happy and stroked its tail as the other animals in it. The little girl and the girl became friends and friends. They all was very happy together and had a great that day on they made the race in their cage.<|endoftext|>Once upon a time, there was a little boy named Timmy. Timmy loved class all day long. He lived and were all happy with everything and interesting things. One day, Timmy took a big meal to play with his friends in his family. They had'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Once upon a time, there was a good man. He lived in a big house with his dog. One day, the man heard a loud\"\n",
    "ids = enc.encode_ordinary(sentence)\n",
    "new_ids = remap_ids(ids)\n",
    "context = torch.tensor([new_ids], dtype=torch.long, device=device)\n",
    "y = model.generate(context, 200)\n",
    "orig_ids = [rev_map[i] for i in y.squeeze().tolist()]\n",
    "output = enc.decode(orig_ids)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "091a7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"model_42mb_result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f37dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 144 mb\n",
    "# 1 Grammar: 5/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5)\n",
    "# 2 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 3 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 4 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 5 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 6 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 7 Grammar: 6/10, Creativity: 7/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 8 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 9 Grammar: 6/10, Creativity: 7/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 10 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 67 mb\n",
    "# 1 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 2 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 3 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 4 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 5 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 6 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 7 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 8 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 9 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 10 Grammar: 3/10, Creativity: 6/10, Consistency: 2/10, Age group: B (4-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3d88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47.3 mb\n",
    "# 1 Grammar: 4/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5) Smaller\n",
    "# 2 Grammar: 5/10, Creativity: 5/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 3 Grammar: 4/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5)\n",
    "# 4 Grammar: 3/10, Creativity: 5/10, Consistency: 2/10, Age group: B (4-5)\n",
    "# 5 Grammar: 4/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 6 Grammar: 3/10, Creativity: 5/10, Consistency: 2/10, Age group: B (4-5) Token errors - Error with vocab\n",
    "# 7 Grammar: 4/10, Creativity: 5/10, Consistency: 3/10, Age group: B (4-5) Smaller\n",
    "# 8 Grammar: 4/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 9 Grammar: 4/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 10 Grammar: 5/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5) Smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47.3 mb second/third try\n",
    "# 1 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: B (4-5) Smaller\n",
    "# 2 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 3 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 4 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 5 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) \n",
    "# 6 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Token errors (fixed in 3 try)\n",
    "# 7 Grammar: 6/10, Creativity: 5/10, Consistency: 5/10, Age group: B (4-5) Smaller\n",
    "# 8 Grammar: 4/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) \n",
    "# 9 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 10 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: B (4-5) Smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb26fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44.8 mb\n",
    "# 1 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5)\n",
    "# 2 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 3 Grammar: 4/10, Creativity: 5/10, Consistency: 3/10, Age group: B (4-5) Very Smaller\n",
    "# 4 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 5 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 6 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 7 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 8 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: B (4-5) Smaller\n",
    "# 9 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 10 Grammar: 4/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5) Smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945be132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43.1 mb\n",
    "# 1 Grammar: 5/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Very Smaller\n",
    "# 2 Grammar: 4/10, Creativity: 6/10, Consistency: 3/10, Age group: B (4-5)\n",
    "# 3 Grammar: 5/10, Creativity: 5/10, Consistency: 4/10, Age group: C (6-7) Medium Smaller\n",
    "# 4 Grammar: 5/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Smaller\n",
    "# 5 Grammar: 5/10, Creativity: 6/10, Consistency: 4/10, Age group: C (6-7) Smaller\n",
    "# 6 Grammar: 4/10, Creativity: 5/10, Consistency: 3/10, Age group: B (4-5) Medium Smaller\n",
    "# 7 Grammar: 2/10, Creativity: 4/10, Consistency: 2/10, Age group: A (3 or under) Smaller\n",
    "# 8 Grammar: 4/10, Creativity: 5/10, Consistency: 4/10, Age group: B (4-5) Very Smaller\n",
    "# 9 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Smaller\n",
    "# 10 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b947248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42.1 mb (Second try fixed vocab)\n",
    "# 1 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Medium Smaller\n",
    "# 2 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Medium Smaller\n",
    "# 3 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Second try Smaller\n",
    "# 4 Grammar: 7/10, Creativity: 6/10, Consistency: 6/10, Age group: C (6-7) Medium Smaller\n",
    "# 5 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7)\n",
    "# 6 Grammar: 7/10, Creativity: 7/10, Consistency: 6/10, Age group: C (6-7) Smaller\n",
    "# 7 Grammar: 7/10, Creativity: 7/10, Consistency: 6/10, Age group: C (6-7) Medium Smaller\n",
    "\n",
    "# 8 Grammar: 3/10, Creativity: 5/10, Consistency: 3/10, Age group: B (4-5) Smaller - Another GPTChat, another answer...\n",
    "# 8 Grammar: 6/10, Creativity: 6/10, Consistency: 5/10, Age group: C (6-7) Another GPTChat, another answer...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d774f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
