{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a9f843",
   "metadata": {
    "executionInfo": {
     "elapsed": 9458,
     "status": "ok",
     "timestamp": 1753204059134,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "f0a9f843"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oTIJaMb46vx8",
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1753204078270,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "oTIJaMb46vx8"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "FPK7Np-0Sc-h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1753204081690,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "FPK7Np-0Sc-h",
    "outputId": "cc50a54e-5c65-4a46-bb7c-bde5bf5736f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "U2fCyrEU6XCW",
   "metadata": {
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1753204083841,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "U2fCyrEU6XCW"
   },
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nz4Txftb6esb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1753204155915,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "nz4Txftb6esb",
    "outputId": "3a4baa7c-99d6-4d2f-ba98-4abcf2b757ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 384)\n",
       "    (wpe): Embedding(128, 384)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=384, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model_params_20000.pt\"))\n",
    "model.to(\"cuda\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a4360d",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1753204157629,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "f8a4360d"
   },
   "outputs": [],
   "source": [
    "with open(\"fridge_dataset_v1.3_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ef0da0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1753204159256,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "a7ef0da0",
    "outputId": "d8f8cce4-e713-4e89-a5a3-96e6ea1c6e3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Where can consumers find detailed replacement instructions for lamps and control gear?', 'For detailed instructions on replacing lamps and control gear, consumers should visit the Samsung website and navigate to the \"Support\" section. By entering the model name, users can access specific guidance. Professional support is recommended as these components are not user-serviceable.')\n"
     ]
    }
   ],
   "source": [
    "# Можна спростити до input-output пари\n",
    "pairs = [(item[\"instruction\"], item[\"response\"]) for item in data]\n",
    "print(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8357ed5d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1596,
     "status": "ok",
     "timestamp": 1753204162535,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "8357ed5d"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "block_size = config.block_size  # =128\n",
    "\n",
    "def encode_pair(instruction, response):\n",
    "    prompt = f\"question: {instruction}\\nanswer: {response}<|END|>\"\n",
    "    tokens = enc.encode_ordinary(prompt)\n",
    "    if len(tokens) > block_size - 1:\n",
    "        tokens = tokens[:block_size - 1] + [50256]  # Додаємо <|endoftext|> як токен кінця\n",
    "    else:\n",
    "        tokens = tokens + [50256] + [0] * (block_size - len(tokens) - 1)  # Паддінг до block_size\n",
    "    x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "    y = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4yT2ui1TY_p5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 162,
     "status": "ok",
     "timestamp": 1753204162842,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "4yT2ui1TY_p5",
    "outputId": "9862e001-9f8f-4790-9d2c-ed6d4129569f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Could you provide the frequency bands and maximum ... has 143 tokens, exceeds block_size=128\n",
      "Instruction: Which methods are suggested for cleaning the refri... has 136 tokens, exceeds block_size=128\n",
      "Instruction: How can one prevent the appliance from causing foo... has 146 tokens, exceeds block_size=128\n",
      "Instruction: How can food contamination be prevented in the app... has 141 tokens, exceeds block_size=128\n",
      "Instruction: What strategies can be implemented to prevent cont... has 152 tokens, exceeds block_size=128\n",
      "Instruction: Could you provide the frequency bands and maximum ... has 141 tokens, exceeds block_size=128\n",
      "Instruction: How can one prevent the appliance from causing foo... has 142 tokens, exceeds block_size=128\n",
      "Instruction: How is it possible to stop food from becoming cont... has 149 tokens, exceeds block_size=128\n",
      "Instruction: Can you specify the frequency ranges and highest o... has 135 tokens, exceeds block_size=128\n",
      "Instruction: What measures can be taken to avoid food contamina... has 144 tokens, exceeds block_size=128\n",
      "Instruction: How can one effectively clean the inside and outsi... has 135 tokens, exceeds block_size=128\n",
      "Instruction: What are the operating frequency ranges and maximu... has 136 tokens, exceeds block_size=128\n",
      "Instruction: How should the inside and outside of the refrigera... has 134 tokens, exceeds block_size=128\n",
      "Instruction: What are the best practices for cleaning the inter... has 136 tokens, exceeds block_size=128\n",
      "Instruction: What are the operating frequency ranges and maximu... has 140 tokens, exceeds block_size=128\n",
      "Instruction: What strategies can be implemented to prevent cont... has 145 tokens, exceeds block_size=128\n",
      "Instruction: For the equipment, what frequency ranges and maxim... has 138 tokens, exceeds block_size=128\n",
      "Instruction: What measures can be taken to avoid food contamina... has 151 tokens, exceeds block_size=128\n",
      "Instruction: What measures can be taken to avoid food contamina... has 144 tokens, exceeds block_size=128\n",
      "Instruction: For the equipment, what frequency ranges and maxim... has 140 tokens, exceeds block_size=128\n",
      "Instruction: What are the recommended cleaning methods for the ... has 136 tokens, exceeds block_size=128\n",
      "Instruction: Could you provide the frequency bands and maximum ... has 135 tokens, exceeds block_size=128\n",
      "Instruction: What are the best practices for cleaning the inter... has 136 tokens, exceeds block_size=128\n",
      "Instruction: Can you specify the frequency ranges and highest o... has 139 tokens, exceeds block_size=128\n",
      "Instruction: What are the frequency ranges and maximum transmit... has 131 tokens, exceeds block_size=128\n",
      "Instruction: How can food contamination be prevented in the app... has 148 tokens, exceeds block_size=128\n",
      "Instruction: For the equipment, what frequency ranges and maxim... has 132 tokens, exceeds block_size=128\n",
      "Instruction: What strategies can be implemented to prevent cont... has 149 tokens, exceeds block_size=128\n",
      "Instruction: What frequency bands and peak transmitter power le... has 138 tokens, exceeds block_size=128\n",
      "Instruction: What cleaning techniques are advised for both the ... has 137 tokens, exceeds block_size=128\n",
      "Instruction: What are the recommended cleaning methods for the ... has 136 tokens, exceeds block_size=128\n",
      "Instruction: Can you specify the frequency ranges and highest o... has 141 tokens, exceeds block_size=128\n",
      "Instruction: Which methods are suggested for cleaning the refri... has 136 tokens, exceeds block_size=128\n",
      "Instruction: What are the operating frequency ranges and maximu... has 132 tokens, exceeds block_size=128\n",
      "Instruction: What steps should be followed to ensure food remai... has 151 tokens, exceeds block_size=128\n",
      "Instruction: How can food contamination be prevented in the app... has 145 tokens, exceeds block_size=128\n",
      "Instruction: How should the inside and outside of the refrigera... has 134 tokens, exceeds block_size=128\n",
      "Instruction: What steps should be followed to ensure food remai... has 147 tokens, exceeds block_size=128\n",
      "Instruction: What strategies can be implemented to prevent cont... has 145 tokens, exceeds block_size=128\n",
      "Instruction: How can one prevent the appliance from causing foo... has 142 tokens, exceeds block_size=128\n",
      "Instruction: For the equipment, what frequency ranges and maxim... has 136 tokens, exceeds block_size=128\n",
      "Instruction: What cleaning techniques are advised for both the ... has 137 tokens, exceeds block_size=128\n",
      "Instruction: What are the operating frequency ranges and maximu... has 138 tokens, exceeds block_size=128\n",
      "Instruction: What steps should be followed to ensure food remai... has 154 tokens, exceeds block_size=128\n",
      "Instruction: How can one effectively clean the inside and outsi... has 135 tokens, exceeds block_size=128\n",
      "Instruction: Could you provide the frequency bands and maximum ... has 139 tokens, exceeds block_size=128\n",
      "Instruction: How is it possible to stop food from becoming cont... has 153 tokens, exceeds block_size=128\n",
      "Instruction: What frequency bands and peak transmitter power le... has 142 tokens, exceeds block_size=128\n",
      "Instruction: What frequency bands and peak transmitter power le... has 134 tokens, exceeds block_size=128\n",
      "Instruction: How can one prevent the appliance from causing foo... has 149 tokens, exceeds block_size=128\n",
      "Instruction: What frequency bands and peak transmitter power le... has 140 tokens, exceeds block_size=128\n",
      "Instruction: How can food contamination be prevented in the app... has 141 tokens, exceeds block_size=128\n",
      "Instruction: What are the frequency ranges and maximum transmit... has 137 tokens, exceeds block_size=128\n",
      "Instruction: How is it possible to stop food from becoming cont... has 149 tokens, exceeds block_size=128\n",
      "Instruction: What are the frequency ranges and maximum transmit... has 135 tokens, exceeds block_size=128\n",
      "Instruction: What are the frequency ranges and maximum transmit... has 139 tokens, exceeds block_size=128\n",
      "Instruction: How is it possible to stop food from becoming cont... has 156 tokens, exceeds block_size=128\n"
     ]
    }
   ],
   "source": [
    "# Аналіз довжини відповідей\n",
    "for item in data:\n",
    "    prompt = f\"question: {item['instruction']}\\nanswer: {item['response']}<|END|>\"\n",
    "    tokens = enc.encode_ordinary(prompt)\n",
    "    if len(tokens) > block_size:\n",
    "        print(f\"Instruction: {item['instruction'][:50]}... has {len(tokens)} tokens, exceeds block_size={block_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5u7tpvjW8cx7",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1753204165814,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "5u7tpvjW8cx7"
   },
   "outputs": [],
   "source": [
    "train_pairs, val_pairs = train_test_split(pairs, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40991768",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1753204167074,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "40991768"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.samples = [encode_pair(instr, resp) for instr, resp in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "MtuY0QOJBl68",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1753204168069,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "MtuY0QOJBl68"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # batch — список із (x, y) пар\n",
    "    # Розпаковуємо\n",
    "    xs, ys = zip(*batch)\n",
    "\n",
    "    # Паддінг input і output (можна паддити токеном 0, або іншим padding_id)\n",
    "    xs_padded = torch.nn.utils.rnn.pad_sequence(xs, batch_first=True, padding_value=0)\n",
    "    ys_padded = torch.nn.utils.rnn.pad_sequence(ys, batch_first=True, padding_value=0)\n",
    "\n",
    "    return xs_padded, ys_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "marOrY6x8mY7",
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1753204169270,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "marOrY6x8mY7"
   },
   "outputs": [],
   "source": [
    "train_dataset = InstructionDataset(train_pairs)\n",
    "val_dataset = InstructionDataset(val_pairs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9Sv1BIkU8uzp",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1753204170257,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "9Sv1BIkU8uzp"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "            logits, loss = model(x, y)\n",
    "            total_loss += loss.item()\n",
    "    model.train()\n",
    "    return total_loss / len(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "-gzYWSTp8vfP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1175203,
     "status": "ok",
     "timestamp": 1753205346690,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "-gzYWSTp8vfP",
    "outputId": "f890f6a3-bb0e-4186-8d07-59fd5745c128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 3.1222, val loss 2.3109\n",
      "Epoch 2: train loss 2.1530, val loss 1.8384\n",
      "Epoch 3: train loss 1.7981, val loss 1.5687\n",
      "Epoch 4: train loss 1.5614, val loss 1.3416\n",
      "Epoch 5: train loss 1.3753, val loss 1.1780\n",
      "Epoch 6: train loss 1.2193, val loss 1.0357\n",
      "Epoch 7: train loss 1.0836, val loss 0.9072\n",
      "Epoch 8: train loss 0.9685, val loss 0.8007\n",
      "Epoch 9: train loss 0.8649, val loss 0.7207\n",
      "Epoch 10: train loss 0.7792, val loss 0.6424\n",
      "Epoch 11: train loss 0.7045, val loss 0.5761\n",
      "Epoch 12: train loss 0.6323, val loss 0.5160\n",
      "Epoch 13: train loss 0.5768, val loss 0.4666\n",
      "Epoch 14: train loss 0.5228, val loss 0.4278\n",
      "Epoch 15: train loss 0.4762, val loss 0.3901\n",
      "Epoch 16: train loss 0.4374, val loss 0.3536\n",
      "Epoch 17: train loss 0.3991, val loss 0.3257\n",
      "Epoch 18: train loss 0.3669, val loss 0.2999\n",
      "Epoch 19: train loss 0.3405, val loss 0.2786\n",
      "Epoch 20: train loss 0.3159, val loss 0.2600\n",
      "Epoch 21: train loss 0.2927, val loss 0.2429\n",
      "Epoch 22: train loss 0.2733, val loss 0.2285\n",
      "Epoch 23: train loss 0.2569, val loss 0.2159\n",
      "Epoch 24: train loss 0.2425, val loss 0.2055\n",
      "Epoch 25: train loss 0.2304, val loss 0.1969\n",
      "Epoch 26: train loss 0.2162, val loss 0.1884\n",
      "Epoch 27: train loss 0.2050, val loss 0.1809\n",
      "Epoch 28: train loss 0.1960, val loss 0.1719\n",
      "Epoch 29: train loss 0.1871, val loss 0.1682\n",
      "Epoch 30: train loss 0.1803, val loss 0.1629\n",
      "Epoch 31: train loss 0.1737, val loss 0.1575\n",
      "Epoch 32: train loss 0.1690, val loss 0.1531\n",
      "Epoch 33: train loss 0.1629, val loss 0.1507\n",
      "Epoch 34: train loss 0.1588, val loss 0.1485\n",
      "Epoch 35: train loss 0.1544, val loss 0.1455\n",
      "Epoch 36: train loss 0.1506, val loss 0.1432\n",
      "Epoch 37: train loss 0.1478, val loss 0.1413\n",
      "Epoch 38: train loss 0.1437, val loss 0.1397\n",
      "Epoch 39: train loss 0.1417, val loss 0.1377\n",
      "Epoch 40: train loss 0.1395, val loss 0.1363\n",
      "Epoch 41: train loss 0.1376, val loss 0.1357\n",
      "Epoch 42: train loss 0.1361, val loss 0.1349\n",
      "Epoch 43: train loss 0.1345, val loss 0.1339\n",
      "Epoch 44: train loss 0.1332, val loss 0.1336\n",
      "Epoch 45: train loss 0.1325, val loss 0.1327\n",
      "Epoch 46: train loss 0.1311, val loss 0.1324\n",
      "Epoch 47: train loss 0.1305, val loss 0.1322\n",
      "Epoch 48: train loss 0.1293, val loss 0.1318\n",
      "Epoch 49: train loss 0.1297, val loss 0.1318\n",
      "Epoch 50: train loss 0.1289, val loss 0.1316\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "        logits, loss = model(x, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: train loss {avg_train_loss:.4f}, val loss {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "kIzF40hF84Kd",
   "metadata": {
    "executionInfo": {
     "elapsed": 209,
     "status": "ok",
     "timestamp": 1753205573253,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "kIzF40hF84Kd"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"gpt_1.3_new_gpt_50ep.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "kp3wsiQhE0FT",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1753208478294,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "kp3wsiQhE0FT"
   },
   "outputs": [],
   "source": [
    "def generate_response(instruction, max_new_tokens=100, temperature=0.6, top_k=40):\n",
    "    prompt = f\"question: {instruction}\\nanswer:\"\n",
    "    input_ids = enc.encode_ordinary(prompt)\n",
    "    input_ids = input_ids[:config.block_size]\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long)[None].to(\"cuda\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            if input_tensor.shape[1] > config.block_size:\n",
    "                input_tensor = input_tensor[:, -config.block_size:]\n",
    "\n",
    "            logits, _ = model(input_tensor)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                values, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < values[:, [-1]]] = -float(\"inf\")\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            input_tensor = torch.cat([input_tensor, next_token], dim=1)\n",
    "\n",
    "            # Зупиняємо генерацію, якщо згенеровано токен <|END|> (50256)\n",
    "            if next_token.item() == 50256:\n",
    "                break\n",
    "\n",
    "    output_tokens = input_tensor[0].tolist()\n",
    "    generated = enc.decode(output_tokens[len(input_ids):])\n",
    "    return generated.strip().replace(\"<|END|><|endoftext|>\", \"\")  # Видаляємо <|END|> із виводу\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "DrAWzgGkORKw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 97524,
     "status": "ok",
     "timestamp": 1753206310375,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "DrAWzgGkORKw",
    "outputId": "9c854321-0df3-4f22-ee97-6766964e3f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.53.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.14.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.7.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.33.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.7.14)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert_score) (1.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "mfuftF-iQmxE",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1753206960037,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "mfuftF-iQmxE"
   },
   "outputs": [],
   "source": [
    "from bert_score import score as bert_score\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "WC69N7ayGCqX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6264,
     "status": "ok",
     "timestamp": 1753208492016,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "WC69N7ayGCqX",
    "outputId": "5ca7af19-ba2c-408e-dd18-b9dfb15489f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: What benefits does a multipurpose freezer bin offer in specific models?\n",
      "Expected: Designed to boost the storage capacity of select refrigerators, the multipurpose freezer bin offers adaptable freezer space. It helps users efficiently arrange and access their frozen goods, meeting a range of storage demands.\n",
      "Generated: The multipurpose freezer bin is created to improve storage options in certain refrigerator models by providing flexible freezer space usage. This allows users to better organize and easily reach their frozen items, accommodating various storage requirements.\n",
      "BERTScore F1: 0.9424\n",
      "--------------------------------------------------\n",
      "Instruction: Are there any precautions to take when thawing the appliance?\n",
      "Expected: To avoid harm to the appliance and maintain safe defrosting, do not employ mechanical devices or alternative methods to hasten the process unless explicitly advised by the manufacturer.\n",
      "Generated: Accelerating defrosting with mechanical equipment or other approaches is not recommended unless approved by the manufacturer, as it helps prevent damage and promotes safe defrosting procedures.\n",
      "BERTScore F1: 0.9102\n",
      "--------------------------------------------------\n",
      "Instruction: What is the meaning of the \"Read the instructions\" symbol?\n",
      "Expected: The \"Read the instructions\" icon serves as a reminder for users to review all guidelines prior to operating the appliance and to keep the manual in a secure location for future use. Doing so ensures safe and proper operation throughout the refrigerator's lifetime.\n",
      "Generated: The \"Read the instructions\" icon serves as a reminder for users to review all guidelines prior to operating the appliance and to keep the manual in a secure location for future use. Doing so ensures safe and proper operation throughout the refrigerator's lifetime.\n",
      "BERTScore F1: 1.0000\n",
      "--------------------------------------------------\n",
      "Instruction: Can you explain the rules of cricket?\n",
      "Expected: I apologize, but I am a refrigerator assistant and cannot help with sports rules or explanations.\n",
      "Generated: I apologize, but I am a refrigerator assistant and cannot help with game instructions.\n",
      "BERTScore F1: 0.9682\n",
      "--------------------------------------------------\n",
      "Instruction: What actions are necessary for the refrigerator door to preserve ideal cooling conditions?\n",
      "Expected: The door should be kept closed as much as possible to maintain stable temperatures and prevent temperature fluctuations that can affect food freshness.\n",
      "Generated: Maintaining a closed door as much as possible helps prevent temperature swings, which is important for keeping your food fresh.\n",
      "BERTScore F1: 0.9244\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for instruction, reference in train_pairs[:5]:\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    # Порахувати BERTScore (F1) для цієї пари\n",
    "    P, R, F1 = bert_score([generated], [reference], lang='en', verbose=False)\n",
    "\n",
    "    print(\"Instruction:\", instruction)\n",
    "    print(\"Expected:\", reference)\n",
    "    print(\"Generated:\", generated)\n",
    "    print(f\"BERTScore F1: {F1[0].item():.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V7ykoVN-LZy_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7ykoVN-LZy_",
    "outputId": "4c6dab7b-5f7e-4593-8b62-4378c18c1475"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 947/2833 [02:52<07:08,  4.40it/s]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "# Проходимо по всім прикладам 1\n",
    "for instruction, reference in tqdm(train_pairs):\n",
    "    generated = generate_response(instruction)\n",
    "\n",
    "    results.append({\n",
    "        \"instruction\": instruction,\n",
    "        \"expected\": reference,\n",
    "        \"generated\": generated\n",
    "    })\n",
    "\n",
    "# Запис у JSON-файл\n",
    "with open(\"chatgpt_1.3_new_gpt_train_pred.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Збережено в gpt_train_pred.json\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# Проходимо по всім прикладам 2\n",
    "for instruction, reference in tqdm(val_pairs):\n",
    "    generated = generate_response(instruction)\n",
    "    P, R, F1 = bert_score([generated], [reference], lang='en', verbose=False)\n",
    "\n",
    "    results.append({\n",
    "        \"instruction\": instruction,\n",
    "        \"expected\": reference,\n",
    "        \"generated\": generated,\n",
    "        \"BERTScore\": f\"{F1[0].item():.4f}\"\n",
    "    })\n",
    "\n",
    "# Запис у JSON-файл\n",
    "with open(\"chatgpt_1.3_new_gpt_val_pred.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Збережено в gpt_val_pred.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8HkS5dlMHK1T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1753208507204,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "8HkS5dlMHK1T",
    "outputId": "093c5844-070e-45df-d689-df5df92db372"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but I am a refrigerator assistant and cannot help with cooking techniques.\n",
      "I apologize, but I am a refrigerator assistant and cannot help with literary analysis.\n",
      "I apologize, but I am a refrigerator assistant and cannot help with literary advice.\n",
      "I apologize, but I am a refrigerator assistant and cannot help with sports rules.\n",
      "I apologize, but I am a refrigerator assistant and cannot help with medical or literary processes.\n"
     ]
    }
   ],
   "source": [
    "print(generate_response(\"How can I make a salad?\"))\n",
    "print(generate_response(\"What is the capital of France?\"))\n",
    "print(generate_response(\"Explain the theory of relativity in simple terms.\"))\n",
    "print(generate_response(\"What are the benefits of regular exercise?\"))\n",
    "print(generate_response(\"What is the process of photosynthesis?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p9AxzlKrGABf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92213,
     "status": "ok",
     "timestamp": 1753208617512,
     "user": {
      "displayName": "Maxym",
      "userId": "15983387890919333918"
     },
     "user_tz": -180
    },
    "id": "p9AxzlKrGABf",
    "outputId": "2cdf6b2f-2fd5-4546-c867-adaa323c4899"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:29<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8100\n",
      "Recall:    0.8097\n",
      "F1 Score:  0.8095\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_samples = val_pairs\n",
    "\n",
    "references = [ex[1] for ex in test_samples]  # response\n",
    "candidates = [generate_response(ex[0]) for ex in tqdm(test_samples)]  # instruction\n",
    "\n",
    "\n",
    "P, R, F1 = score(\n",
    "    candidates,\n",
    "    references,\n",
    "    lang=\"en\",\n",
    "    # model_type=\"bert-base-uncased\",\n",
    "    device=\"cuda\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "print(f\"Precision: {P.mean().item():.4f}\")\n",
    "print(f\"Recall:    {R.mean().item():.4f}\")\n",
    "print(f\"F1 Score:  {F1.mean().item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
