{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0a9f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "import json\n",
    "import evaluate\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8a4360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"samsung_refrigerator_qa.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7ef0da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'context': 'Install the appliance on a firm and level floor. Do not install the appliance in a damp and dusty place. Do not install or store the appliance in any outdoor area, or any area that is subject to weathering conditions such as direct sunlight, wind, rain, or temperatures below freezing.', 'question': 'Where should the appliance be installed?', 'answers': [{'answer_start': 24, 'text': 'on a firm and level floor'}]}\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for item in data[\"data\"]:\n",
    "    for para in item[\"paragraphs\"]:\n",
    "        context = para[\"context\"]\n",
    "        for qa in para[\"qas\"]:\n",
    "            row = {\n",
    "                \"id\": str(len(rows)),\n",
    "                \"context\": context,\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answers\": qa[\"answers\"]  # список з текстом і стартом відповіді\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "# Створюємо Dataset\n",
    "dataset = Dataset.from_list(rows)\n",
    "\n",
    "print(dataset[0])  # Перевірка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8357ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Крок 2: Ініціалізація токенізатора та моделі ---\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c8d30089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # Додаємо example_id, щоб потім зв'язати з оригіналом\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    # examples['id'] — список списків, розгорнемо в один список\n",
    "    flat_ids = examples[\"id\"]\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        tokenized_examples[\"example_id\"].append(flat_ids[sample_mapping[i]])\n",
    "\n",
    "        \n",
    "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # flatten answers аналогічно\n",
    "    flat_answers = [item for sublist in examples[\"answers\"] for item in sublist]\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        example_id = tokenized_examples[\"example_id\"][i]\n",
    "        # example_id — це id прикладу, знаходимо його індекс у flat_ids\n",
    "        sample_index = flat_ids.index(example_id)\n",
    "        answer_list = answers[sample_index]  # це список словників\n",
    "\n",
    "        if len(answer_list) == 0:\n",
    "            start_positions.append(tokenizer.model_max_length)\n",
    "            end_positions.append(tokenizer.model_max_length)\n",
    "        else:\n",
    "            # беремо першу відповідь (якщо їх кілька)\n",
    "            answer = answer_list[0]\n",
    "            start_char = answer[\"answer_start\"]\n",
    "            end_char = start_char + len(answer[\"text\"])\n",
    "\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "            token_end_index = len(offsets) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                start_positions.append(tokenizer.model_max_length)\n",
    "                end_positions.append(tokenizer.model_max_length)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06bedba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/52 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 52/52 [00:00<00:00, 1243.23 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 728.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Спочатку робиш розбиття на train/test для оригінального (сирого) датасету:\n",
    "split_dataset = dataset.train_test_split(test_size=0.15, seed=42)\n",
    "\n",
    "tokenized_train = split_dataset[\"train\"].map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in split_dataset[\"train\"].column_names if col not in (\"id\", \"example_id\")]\n",
    ")\n",
    "\n",
    "tokenized_test = split_dataset[\"test\"].map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    remove_columns=[col for col in split_dataset[\"test\"].column_names if col not in (\"id\", \"example_id\")]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1133424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всі example_id з features присутні у examples\n"
     ]
    }
   ],
   "source": [
    "# Припустимо, examples і features — це об'єкти datasets.Dataset\n",
    "examples = split_dataset[\"test\"]          # Оригінальні приклади (мають id)\n",
    "features = tokenized_test                  # Токенізовані фічі (мають example_id)\n",
    "\n",
    "# Побудова словника для швидкого пошуку індексів\n",
    "example_id_to_index = {str(k): i for i, k in enumerate(examples[\"id\"])}\n",
    "\n",
    "# Перевірка, чи всі example_id з features є в examples\n",
    "missing_ids = set()\n",
    "for feature in features:\n",
    "    eid = str(feature[\"example_id\"])\n",
    "    if eid not in example_id_to_index:\n",
    "        missing_ids.add(eid)\n",
    "\n",
    "if missing_ids:\n",
    "    print(f\"Відсутні example_id у examples: {missing_ids}\")\n",
    "else:\n",
    "    print(\"Всі example_id з features присутні у examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70e50638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'input_ids', 'attention_mask', 'offset_mapping', 'example_id', 'start_positions', 'end_positions']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_test.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ccb3cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Крок 4: Постпроцесинг для обчислення текстових відповідей ---\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "\n",
    "    example_id_to_index = {str(k): i for i, k in enumerate(examples[\"id\"])}  # <- сюди додано str()\n",
    "\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        example_id = str(feature[\"example_id\"])  # теж приводимо до рядка\n",
    "        if example_id not in example_id_to_index:\n",
    "            print(f\"Missing example_id in dict: {example_id}\")\n",
    "        features_per_example[example_id_to_index[example_id]].append(i)\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    for example_index, example in enumerate(examples):\n",
    "        context = example[\"context\"]\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        prelim_predictions = []\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logits)[-1: -n_best_size - 1: -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1: -n_best_size - 1: -1].tolist()\n",
    "\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping):\n",
    "                        continue\n",
    "                    if offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index or (end_index - start_index + 1) > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    text = context[start_char:end_char]\n",
    "\n",
    "                    prelim_predictions.append({\n",
    "                        \"text\": text,\n",
    "                        \"start_logit\": start_logits[start_index],\n",
    "                        \"end_logit\": end_logits[end_index]\n",
    "                    })\n",
    "\n",
    "        if prelim_predictions:\n",
    "            best_pred = max(prelim_predictions, key=lambda x: x[\"start_logit\"] + x[\"end_logit\"])\n",
    "            predictions[example[\"id\"]] = best_pred[\"text\"]\n",
    "        else:\n",
    "            predictions[example[\"id\"]] = \"\"\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9001241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Крок 5: Функція для обчислення метрик ---\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    examples = split_dataset[\"test\"]\n",
    "    features = tokenized_test\n",
    "\n",
    "    preds = postprocess_qa_predictions(examples, features, logits)\n",
    "\n",
    "    formatted_preds = [{\"id\": k, \"prediction_text\": v} for k, v in preds.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "\n",
    "    return metric.compute(predictions=formatted_preds, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "476c3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Крок 6: Параметри тренування ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_finetuned\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7894ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nobla\\AppData\\Local\\Temp\\ipykernel_16412\\2147146533.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# --- Крок 7: Ініціалізація тренера ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c839933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.283200</td>\n",
       "      <td>5.391767</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.382000</td>\n",
       "      <td>4.731651</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.493800</td>\n",
       "      <td>4.244206</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.768600</td>\n",
       "      <td>3.792037</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.972200</td>\n",
       "      <td>3.433322</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>58.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.161700</td>\n",
       "      <td>3.204514</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>58.508159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.207100</td>\n",
       "      <td>3.043259</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>55.841492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.156600</td>\n",
       "      <td>2.946510</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>55.841492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.225400</td>\n",
       "      <td>2.884795</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>55.841492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.442600</td>\n",
       "      <td>2.839820</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.479200</td>\n",
       "      <td>2.811879</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.580300</td>\n",
       "      <td>2.794353</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.736100</td>\n",
       "      <td>2.784390</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.842800</td>\n",
       "      <td>2.778763</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14, training_loss=3.766544818878174, metrics={'train_runtime': 57.9011, 'train_samples_per_second': 1.796, 'train_steps_per_second': 0.242, 'total_flos': 10190941802496.0, 'train_loss': 3.766544818878174, 'epoch': 2.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Крок 8: Запуск тренування ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec505066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'input_ids', 'attention_mask', 'offset_mapping', 'example_id', 'start_positions', 'end_positions']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_test.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e76ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Крок 7: Збереження моделі ---\n",
    "trainer.save_model(\"./bert_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
