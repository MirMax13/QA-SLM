{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a9f843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LLM\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering,Trainer, TrainingArguments\n",
    "import torch\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a4360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datasets/ChatGPT/extractive/fridge_dataset_v1.0_clean.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ef0da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Maintenance\\nCleaning\\nInterior and exterior\\nвљ\\xa0 WARNING\\nвЂў Do not use benzene, thinner, vinegar, liquid bleach, baking soda, citric, salt mixtures or home/car detergent such as Cloroxв„ў for cleaning purposes. They may damage the surface of the refrigerator and cause a fire.\\nвЂў Do not spray water onto the refrigerator. This may cause electric shock.\\nRegularly use a dry cloth to remove all foreign substances such as dust or water from the power plug terminals and contact points.\\n1. Unplug the power cord.\\n2. Use a moistened, soft, lint-free cloth or paper towel to clean the refrigeratorвЂ™s interior and exterior.\\n3. When done, use a dry cloth or paper towel to dry well.\\n4. Plug in the power cord.\\n\\nLED Lamps\\nTo replace the lamps of the refrigerator, contact a local Samsung service centre.\\nвљ\\xa0 WARNING\\nThe lamps are not user-serviceable. Do not attempt to replace a lamp yourself. This can cause electric shock.\\n\\nLamp (Light Source)\\nThis product contains a light source of energy efficiency class <G>.\\nThe lamp(s) and/or control gear(s) are not user-serviceable. To replace the lamp(s) and/or control gear(s) in the product, contact a local Samsung service centre.\\nFor detailed instructions on replacing lamp(s) or control gear(s) in your product, visit the Samsung website (http://www.samsung.\\ncom), go to Support > Support home, and then enter the model name.\\nFor detailed instructions on dismantling lamp(s) and/or control gear(s), simply follow the replacement instruction reached as described above.\\n\\nAppendix\\nSafety Instruction\\nвЂў This refrigerating appliance is not intended to be used as a built-in appliance.\\n\\nInformation for model and ordering spare parts\\nModel information\\nTo access energy labelling information about this product on the European Product Registry for Energy Labelling (EPREL), scan QR-Code on the energy label.\\nYou can find QR-Code on the energy label in your product box.\\n\\nPart information\\nвЂў The minimum period during which spare parts, necessary for the repair of the appliance, are available\\nвЂ“ 7 Years thermostats, temperature sensors, printed circuit boards and light sources, door handles, door hinges, trays, baskets (boxes or drawers)\\n- 10 Years door gaskets\\nвЂў The minimum duration of the guarantee of the refrigerating appliance offered by the manufacturer 24 Months.\\nвЂў Relevant information for ordering spare parts, directly or through other channels provided by the manufacturer, importer or authorized representative\\nвЂў You can find professional repair information on http://samsung.\\ncom/support.\\nвЂў You can find user servicing manual on http://samsung.com/\\nsupport.\\n\\nInstall Instruction\\nFor refrigerating appliances with climate class\\nвЂў Depending on the climate class, this refrigerating appliance is intended to be used at ambient temperatures range as specified following table.\\nвЂў The climate class can be found on the rating plate. The product may not operate properly at temperatures outside of the specific range.\\nвЂў You can find climate class on label inside of your refrigerator', 'question': 'Can you help me book a flight to Paris?', 'answers': {'answer_start': [0], 'text': ['']}, 'is_impossible': True}\n"
     ]
    }
   ],
   "source": [
    "# Перетворення під BERT-формат\n",
    "rows = []\n",
    "for item in data:\n",
    "    context = item[\"context\"]\n",
    "    question = item[\"question\"]\n",
    "    if item[\"answers\"]:\n",
    "        answer_start = item[\"answers\"][0][\"answer_start\"]\n",
    "        answer = item[\"answers\"][0][\"text\"]\n",
    "        rows.append({\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answers\": {\"text\": [answer], \"answer_start\": [answer_start]},\n",
    "            \"is_impossible\": item[\"is_impossible\"]\n",
    "        })\n",
    "    else:\n",
    "        # Якщо відповіді немає, можна пропустити запис або додати порожні значення\n",
    "        rows.append({\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answers\": {\"text\": [\"\"], \"answer_start\": [0]},\n",
    "            \"is_impossible\": item[\"is_impossible\"]\n",
    "        })\n",
    "\n",
    "# Створення Dataset\n",
    "dataset = Dataset.from_list(rows)\n",
    "\n",
    "# Перевірка\n",
    "print(dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8357ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Тренувальний/валідаційний спліт\n",
    "split_dataset = dataset.train_test_split(test_size=0.15, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d30089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\LLM\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\nobla\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# 4. Токенізатор і модель\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06bedba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 76.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Приклад токенізації для першого запису:\n",
      "{'context': 'Instructions about the WEEE\\nCorrect Disposal of This Product (Waste Electrical & Electronic Equipment)\\n(Applicable in countries with separate collection systems)\\nThis marking on the product, accessories or literature indicates that the product and its electronic accessories (e.g. charger, headset, USB cable) should not be disposed of with other household waste at the end of their working life.\\nTo prevent possible harm to the environment or human health from uncontrolled waste disposal, please separate these items from other types of waste and recycle them responsibly to promote the sustainable reuse of material resources.\\nHousehold users should contact either the retailer where they purchased this product, or their local government office, for details of where and how they can take these items for environmentally safe recycling.\\nBusiness users should contact their supplier and check the terms and conditions of the purchase contract. This product and its electronic accessories should not be mixed with other commercial wastes for disposal.\\nFor information on SamsungвЂ™s environmental commitments and product regulatory obligations, e.g. REACH, WEEE or Batteries, visit our sustainability page available via www.samsung.com\\n(For products sold in European countries and in the UK only)', 'question': 'What is the recommended disposal procedure for business users of this product?', 'answers': {'answer_start': [841], 'text': ['Business users should contact their supplier and check the terms and conditions of the purchase contract. This product and its electronic accessories should not be mixed with other commercial wastes for disposal.']}, 'is_impossible': False, 'input_ids': [101, 2054, 2003, 1996, 6749, 13148, 7709, 2005, 2449, 5198, 1997, 2023, 4031, 1029, 102, 8128, 2055, 1996, 16776, 2063, 6149, 13148, 1997, 2023, 4031, 1006, 5949, 5992, 1004, 4816, 3941, 1007, 1006, 12711, 1999, 3032, 2007, 3584, 3074, 3001, 1007, 2023, 10060, 2006, 1996, 4031, 1010, 16611, 2030, 3906, 7127, 2008, 1996, 4031, 1998, 2049, 4816, 16611, 1006, 1041, 1012, 1043, 1012, 3715, 2099, 1010, 4641, 3388, 1010, 18833, 5830, 1007, 2323, 2025, 2022, 21866, 1997, 2007, 2060, 4398, 5949, 2012, 1996, 2203, 1997, 2037, 2551, 2166, 1012, 2000, 4652, 2825, 7386, 2000, 1996, 4044, 2030, 2529, 2740, 2013, 4895, 8663, 13181, 11001, 5949, 13148, 1010, 3531, 3584, 2122, 5167, 2013, 2060, 4127, 1997, 5949, 1998, 28667, 2100, 14321, 2068, 24501, 26029, 5332, 6321, 2000, 5326, 1996, 9084, 2128, 8557, 1997, 3430, 4219, 1012, 4398, 5198, 2323, 3967, 2593, 1996, 20196, 2073, 2027, 4156, 2023, 4031, 1010, 2030, 2037, 2334, 2231, 2436, 1010, 2005, 4751, 1997, 2073, 1998, 2129, 2027, 2064, 2202, 2122, 5167, 2005, 25262, 3647, 17874, 1012, 2449, 5198, 2323, 3967, 2037, 17024, 1998, 4638, 1996, 3408, 1998, 3785, 1997, 1996, 5309, 3206, 1012, 2023, 4031, 1998, 2049, 4816, 16611, 2323, 2025, 2022, 3816, 2007, 2060, 3293, 5949, 2015, 2005, 13148, 1012, 2005, 2592, 2006, 19102, 25529, 29758, 30108, 2015, 4483, 17786, 1998, 4031, 10738, 14422, 1010, 1041, 1012, 1043, 1012, 3362, 1010, 16776, 2063, 2030, 10274, 1010, 3942, 2256, 15169, 3931, 2800, 3081, 7479, 1012, 19102, 1012, 4012, 1006, 2005, 3688, 2853, 1999, 2647, 3032, 1998, 1999, 1996, 2866, 2069, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'start_positions': 156, 'end_positions': 190}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess(examples):\n",
    "    # Tokenize questions and contexts\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None  # Remove return_tensors=\"pt\" for batched processing\n",
    "    )\n",
    "    \n",
    "    # Get start positions and answer texts\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # Process each example in the batch\n",
    "    for i in range(len(examples[\"question\"])):\n",
    "        answer_start = examples[\"answers\"][i][\"answer_start\"][0]\n",
    "        answer_text = examples[\"answers\"][i][\"text\"][0]\n",
    "        \n",
    "        # Get offsets for this example\n",
    "        tokenized_context = tokenizer(\n",
    "            examples[\"context\"][i],\n",
    "            return_offsets_mapping=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        \n",
    "        offsets = tokenized_context[\"offset_mapping\"]\n",
    "        \n",
    "        # Find start/end token indices\n",
    "        start_idx = None\n",
    "        end_idx = None\n",
    "        \n",
    "        for idx, (start, end) in enumerate(offsets):\n",
    "            if start <= answer_start < end:\n",
    "                start_idx = idx\n",
    "                break\n",
    "                \n",
    "        if start_idx is not None:\n",
    "            for idx, (start, end) in enumerate(offsets[start_idx:], start=start_idx):\n",
    "                if end >= answer_start + len(answer_text):\n",
    "                    end_idx = idx\n",
    "                    break\n",
    "                    \n",
    "        if start_idx is None or end_idx is None:\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "            \n",
    "        start_positions.append(start_idx)\n",
    "        end_positions.append(end_idx)\n",
    "    \n",
    "    # Add start and end positions to inputs\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "# Test with a single example\n",
    "single_example = split_dataset[\"train\"].select([0])\n",
    "tokenized_example = single_example.map(preprocess, batched=True)\n",
    "\n",
    "# Print result\n",
    "print(\"Приклад токенізації для першого запису:\")\n",
    "print(tokenized_example[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c21cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Tokens:\n",
      "['business', 'users', 'should', 'contact', 'their', 'supplier', 'and', 'check', 'the', 'terms', 'and', 'conditions', 'of', 'the', 'purchase', 'contract', '.', 'this', 'product', 'and', 'its', 'electronic', 'accessories', 'should', 'not', 'be', 'mixed', 'with', 'other', 'commercial', 'waste', '##s', 'for', 'disposal', '.']\n"
     ]
    }
   ],
   "source": [
    "def check_tokenization(example, start_idx, end_idx):\n",
    "    # Токенізація контексту для виведення\n",
    "    tokenized_context = tokenizer(\n",
    "        example[\"context\"],\n",
    "        return_offsets_mapping=True,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Отримуємо токени контексту\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_context[\"input_ids\"])\n",
    "\n",
    "    # Виводимо контекст між start і end індексами\n",
    "    print(\"Context Tokens:\")\n",
    "    print(tokens[start_idx:end_idx+1])  # Вивести частину токенів відповіді\n",
    "\n",
    "# Перевірка для першого запису\n",
    "check_tokenization(single_example[0], 156, 190)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ec3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_train = split_dataset[\"train\"].map(preprocess, batched=True)\n",
    "tokenized_val = split_dataset[\"test\"].map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce556bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Instructions about the WEEE\\nCorrect Disposal of This Product (Waste Electrical & Electronic Equipment)\\n(Applicable in countries with separate collection systems)\\nThis marking on the product, accessories or literature indicates that the product and its electronic accessories (e.g. charger, headset, USB cable) should not be disposed of with other household waste at the end of their working life.\\nTo prevent possible harm to the environment or human health from uncontrolled waste disposal, please separate these items from other types of waste and recycle them responsibly to promote the sustainable reuse of material resources.\\nHousehold users should contact either the retailer where they purchased this product, or their local government office, for details of where and how they can take these items for environmentally safe recycling.\\nBusiness users should contact their supplier and check the terms and conditions of the purchase contract. This product and its electronic accessories should not be mixed with other commercial wastes for disposal.\\nFor information on SamsungвЂ™s environmental commitments and product regulatory obligations, e.g. REACH, WEEE or Batteries, visit our sustainability page available via www.samsung.com\\n(For products sold in European countries and in the UK only)', 'question': 'What is the recommended disposal procedure for business users of this product?', 'answers': {'answer_start': [841], 'text': ['Business users should contact their supplier and check the terms and conditions of the purchase contract. This product and its electronic accessories should not be mixed with other commercial wastes for disposal.']}, 'is_impossible': False, 'input_ids': [101, 2054, 2003, 1996, 6749, 13148, 7709, 2005, 2449, 5198, 1997, 2023, 4031, 1029, 102, 8128, 2055, 1996, 16776, 2063, 6149, 13148, 1997, 2023, 4031, 1006, 5949, 5992, 1004, 4816, 3941, 1007, 1006, 12711, 1999, 3032, 2007, 3584, 3074, 3001, 1007, 2023, 10060, 2006, 1996, 4031, 1010, 16611, 2030, 3906, 7127, 2008, 1996, 4031, 1998, 2049, 4816, 16611, 1006, 1041, 1012, 1043, 1012, 3715, 2099, 1010, 4641, 3388, 1010, 18833, 5830, 1007, 2323, 2025, 2022, 21866, 1997, 2007, 2060, 4398, 5949, 2012, 1996, 2203, 1997, 2037, 2551, 2166, 1012, 2000, 4652, 2825, 7386, 2000, 1996, 4044, 2030, 2529, 2740, 2013, 4895, 8663, 13181, 11001, 5949, 13148, 1010, 3531, 3584, 2122, 5167, 2013, 2060, 4127, 1997, 5949, 1998, 28667, 2100, 14321, 2068, 24501, 26029, 5332, 6321, 2000, 5326, 1996, 9084, 2128, 8557, 1997, 3430, 4219, 1012, 4398, 5198, 2323, 3967, 2593, 1996, 20196, 2073, 2027, 4156, 2023, 4031, 1010, 2030, 2037, 2334, 2231, 2436, 1010, 2005, 4751, 1997, 2073, 1998, 2129, 2027, 2064, 2202, 2122, 5167, 2005, 25262, 3647, 17874, 1012, 2449, 5198, 2323, 3967, 2037, 17024, 1998, 4638, 1996, 3408, 1998, 3785, 1997, 1996, 5309, 3206, 1012, 2023, 4031, 1998, 2049, 4816, 16611, 2323, 2025, 2022, 3816, 2007, 2060, 3293, 5949, 2015, 2005, 13148, 1012, 2005, 2592, 2006, 19102, 25529, 29758, 30108, 2015, 4483, 17786, 1998, 4031, 10738, 14422, 1010, 1041, 1012, 1043, 1012, 3362, 1010, 16776, 2063, 2030, 10274, 1010, 3942, 2256, 15169, 3931, 2800, 3081, 7479, 1012, 19102, 1012, 4012, 1006, 2005, 3688, 2853, 1999, 2647, 3032, 1998, 1999, 1996, 2866, 2069, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'start_positions': 156, 'end_positions': 156}\n"
     ]
    }
   ],
   "source": [
    "# Перевірка перших 1 прикладів\n",
    "for i in range(1):\n",
    "    print(tokenized_train[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1133424",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70e50638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\TEMP\\ipykernel_7348\\2797870358.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 6. Тренер\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Навчання\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9001241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Крок 5: Функція для обчислення метрик ---\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    examples = split_dataset[\"test\"]\n",
    "    features = tokenized_test\n",
    "\n",
    "    preds = postprocess_qa_predictions(examples, features, logits)\n",
    "\n",
    "    formatted_preds = [{\"id\": k, \"prediction_text\": v} for k, v in preds.items()]\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "\n",
    "    return metric.compute(predictions=formatted_preds, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "476c3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Крок 6: Параметри тренування ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_finetuned\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    logging_steps=1,\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7894ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nobla\\AppData\\Local\\Temp\\ipykernel_16412\\2147146533.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# --- Крок 7: Ініціалізація тренера ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c839933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.283200</td>\n",
       "      <td>5.391767</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.382000</td>\n",
       "      <td>4.731651</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.493800</td>\n",
       "      <td>4.244206</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>64.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.768600</td>\n",
       "      <td>3.792037</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.972200</td>\n",
       "      <td>3.433322</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>58.303030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.161700</td>\n",
       "      <td>3.204514</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>58.508159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.207100</td>\n",
       "      <td>3.043259</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>55.841492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.156600</td>\n",
       "      <td>2.946510</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>55.841492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.225400</td>\n",
       "      <td>2.884795</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>55.841492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.442600</td>\n",
       "      <td>2.839820</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.479200</td>\n",
       "      <td>2.811879</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.580300</td>\n",
       "      <td>2.794353</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.736100</td>\n",
       "      <td>2.784390</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.842800</td>\n",
       "      <td>2.778763</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>57.919414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14, training_loss=3.766544818878174, metrics={'train_runtime': 57.9011, 'train_samples_per_second': 1.796, 'train_steps_per_second': 0.242, 'total_flos': 10190941802496.0, 'train_loss': 3.766544818878174, 'epoch': 2.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Крок 8: Запуск тренування ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ec505066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'input_ids', 'attention_mask', 'offset_mapping', 'example_id', 'start_positions', 'end_positions']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_test.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e76ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Крок 7: Збереження моделі ---\n",
    "trainer.save_model(\"./bert_finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
