import os
import json
import torch
import numpy as np
from datasets import Dataset
from transformers import (
    BartTokenizer, 
    BartForConditionalGeneration, 
    Seq2SeqTrainer, 
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq,
    TrainerCallback
)

# === –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø ===
MODEL_NAME = "facebook/bart-base"
DATA_FILE = "Full.jsonl"
OUTPUT_DIR = "./bart_finetuned_model"
WANDB_PROJECT = "bart-finetune-meluxina"
TARGET_LOSS = 0.20

# === 1. CALLBACK –î–õ–Ø –ó–£–ü–ò–ù–ö–ò –ü–†–ò LOSS <= 0.20 ===
class StopOnLowLossCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —î 'loss' (—Ü–µ training loss) —É –ª–æ–≥–∞—Ö
        if logs and "loss" in logs:
            current_loss = logs["loss"]
            if current_loss <= TARGET_LOSS:
                print(f"\n\nüõë STOPPING CRITERIA MET: Training Loss {current_loss:.4f} <= {TARGET_LOSS}")
                control.should_training_stop = True

# === 2. –ó–ê–í–ê–ù–¢–ê–ñ–ï–ù–ù–Ø –î–ê–ù–ò–• ===
print("üìÑ Loading data...")
data_rows = []
with open(DATA_FILE, "r", encoding="utf-8") as f:
    for line in f:
        try:
            item = json.loads(line)
            # –§—ñ–ª—å—Ç—Ä—É—î–º–æ "—Å–º—ñ—Ç—Ç—è", —è–∫—â–æ —Ä–∞–ø—Ç–æ–º –∑–∞–ª–∏—à–∏–ª–æ—Å—è
            if item.get("tag") == "irrelevant": 
                continue
                
            question = item.get("instruction", "").strip()
            answer = item.get("response", "").strip()
            
            if question and answer:
                data_rows.append({
                    "input": f"question: {question}", 
                    "output": answer
                })
        except json.JSONDecodeError:
            continue

# –°—Ç–≤–æ—Ä—é—î–º–æ Dataset —ñ –¥—ñ–ª–∏–º–æ –Ω–∞ Train (90%) / Test (10%)
full_dataset = Dataset.from_list(data_rows)
dataset_split = full_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = dataset_split["train"]
eval_dataset = dataset_split["test"]

print(f"‚úÖ Data loaded. Train: {len(train_dataset)}, Test: {len(eval_dataset)}")

# === 3. –¢–û–ö–ï–ù–Ü–ó–ê–¶–Ü–Ø ===
tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)

def preprocess_function(examples):
    inputs = [doc for doc in examples["input"]]
    targets = [doc for doc in examples["output"]]
    
    # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –≤—Ö–æ–¥—ñ–≤
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)
    
    # –¢–æ–∫–µ–Ω—ñ–∑–∞—Ü—ñ—è –≤–∏—Ö–æ–¥—ñ–≤ (labels)
    labels = tokenizer(text_target=targets, max_length=128, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)

# === 4. –ú–û–î–ï–õ–¨ –¢–ê –ú–ï–¢–†–ò–ö–ò ===
model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

# === 5. –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –¢–†–ï–ù–£–í–ê–ù–ù–Ø ===
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    eval_strategy="steps",    # –û—Ü—ñ–Ω—é–≤–∞—Ç–∏ –∫–æ–∂–Ω—ñ N –∫—Ä–æ–∫—ñ–≤
    eval_steps=100,                  # –ß–∞—Å—Ç–æ—Ç–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
    logging_steps=10,               # –ß–∞—Å—Ç–æ—Ç–∞ –∑–∞–ø–∏—Å—É –ª–æ–≥—ñ–≤ (–≤–∞–∂–ª–∏–≤–æ –¥–ª—è –Ω–∞—à–æ–≥–æ Callback!)
    save_steps=200,
    learning_rate=5e-5,
    per_device_train_batch_size=16,  # –ú–æ–∂–Ω–∞ 16, —è–∫—â–æ A100
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=100,             # –°—Ç–∞–≤–∏–º–æ —ñ–∑ –∑–∞–ø–∞—Å–æ–º, Callback –∑—É–ø–∏–Ω–∏—Ç—å —Ä–∞–Ω—ñ—à–µ
    predict_with_generate=True,
    fp16=True,                      # –ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –Ω–∞ GPU
    report_to="wandb",              # –í–º–∏–∫–∞—î–º–æ WandB
    run_name="bart-fridge-run-01",
    load_best_model_at_end=True,    # –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –Ω–∞–π–∫—Ä–∞—â—É –º–æ–¥–µ–ª—å –≤ –∫—ñ–Ω—Ü—ñ
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    processing_class=tokenizer,
    data_collator=data_collator,
    callbacks=[StopOnLowLossCallback()] # <--- –ù–ê–® CALLBACK –¢–£–¢
)

# === 6. –ó–ê–ü–£–°–ö ===
print("üöÄ Starting training...")
trainer.train()

print("üíæ Saving model...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("Done!")